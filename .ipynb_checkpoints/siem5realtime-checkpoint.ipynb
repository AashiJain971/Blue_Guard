{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395d0d14-65d4-4a72-9489-e2d75ff676f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "# from datetime import datetime\n",
    "# from zoneinfo import ZoneInfo\n",
    "\n",
    "# IST = ZoneInfo(\"Asia/Kolkata\")\n",
    "# now_ist = lambda: datetime.now(IST).isoformat()\n",
    "\n",
    "# conn = sqlite3.connect(\"access_logs.db\", check_same_thread=False)\n",
    "# cur  = conn.cursor()\n",
    "\n",
    "# # â”€â”€ Tables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# cur.execute(\"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS logs (\n",
    "#     id        INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "#     ip        TEXT,\n",
    "#     time      TEXT,\n",
    "#     method    TEXT,\n",
    "#     url       TEXT,\n",
    "#     status    INTEGER,\n",
    "#     size      INTEGER,\n",
    "#     agent     TEXT,\n",
    "#     ingest_ts TEXT            -- NEW: arrival timestamp in IST\n",
    "# )\n",
    "# \"\"\")\n",
    "\n",
    "# cur.execute(\"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS advanced_logs (\n",
    "#     id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "#     ip TEXT,\n",
    "#     req_per_min INTEGER,\n",
    "#     unique_urls INTEGER,\n",
    "#     error_rate REAL,\n",
    "#     avg_req_size_bytes REAL,\n",
    "#     method_ratio_post_by_get REAL,\n",
    "#     first_time_of_access TEXT\n",
    "# )\n",
    "# \"\"\")\n",
    "\n",
    "# cur.execute(\"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS ip_eachHour_category (\n",
    "#     id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "#     ip TEXT,\n",
    "#     hour TEXT,\n",
    "#     category TEXT\n",
    "# )\n",
    "# \"\"\")\n",
    "\n",
    "# # â”€â”€ Insert suspicious IP with ingest_ts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# cur.executemany(\"\"\"\n",
    "# INSERT INTO logs (ip, time, method, url, status, size, agent, ingest_ts)\n",
    "# VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "# \"\"\", [\n",
    "#     (\"10.0.0.99\", \"2025-06-21T01:00:00\", \"POST\", \"/login\", 401, 1000, \"MaliciousBot/1.0\", now_ist()),\n",
    "#     (\"10.0.0.99\", \"2025-06-21T01:00:05\", \"POST\", \"/login\", 403, 1200, \"MaliciousBot/1.0\", now_ist()),\n",
    "#     (\"10.0.0.99\", \"2025-06-21T01:00:10\", \"POST\", \"/admin\", 401, 800, \"MaliciousBot/1.0\",  now_ist()),\n",
    "# ])\n",
    "\n",
    "# cur.execute(\"\"\"\n",
    "# INSERT INTO advanced_logs (\n",
    "#     ip, req_per_min, unique_urls, error_rate,\n",
    "#     avg_req_size_bytes, method_ratio_post_by_get, first_time_of_access\n",
    "# ) VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "# \"\"\", (\n",
    "#     \"10.0.0.99\", 200, 20, 0.85, 10000, 5.0,\n",
    "#     datetime(2025, 6, 21, 1, 0, tzinfo=IST).isoformat()\n",
    "# ))\n",
    "\n",
    "# cur.execute(\"\"\"\n",
    "# INSERT INTO ip_eachHour_category (ip, hour, category)\n",
    "# VALUES (?, ?, ?)\n",
    "# \"\"\", (\"10.0.0.99\", \"1-2\", \"ğŸ”´ Credential Stuffing\"))\n",
    "\n",
    "# # â”€â”€ Normal IP with ingest_ts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# cur.executemany(\"\"\"\n",
    "# INSERT INTO logs (ip, time, method, url, status, size, agent, ingest_ts)\n",
    "# VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "# \"\"\", [\n",
    "#     (\"192.168.1.200\", \"2025-06-21T05:00:00\", \"GET\", \"/\",      200, 250, \"NormalUser/1.0\", now_ist()),\n",
    "#     (\"192.168.1.200\", \"2025-06-21T05:00:02\", \"GET\", \"/about\", 200, 300, \"NormalUser/1.0\", now_ist())\n",
    "# ])\n",
    "\n",
    "# cur.execute(\"\"\"\n",
    "# INSERT INTO advanced_logs (\n",
    "#     ip, req_per_min, unique_urls, error_rate,\n",
    "#     avg_req_size_bytes, method_ratio_post_by_get, first_time_of_access\n",
    "# ) VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "# \"\"\", (\n",
    "#     \"192.168.1.200\", 2, 2, 0.0, 275, 0.5,\n",
    "#     datetime(2025, 6, 21, 5, 0, tzinfo=IST).isoformat()\n",
    "# ))\n",
    "\n",
    "# cur.execute(\"\"\"\n",
    "# INSERT INTO ip_eachHour_category (ip, hour, category)\n",
    "# VALUES (?, ?, ?)\n",
    "# \"\"\", (\"192.168.1.200\", \"5-6\", \"ğŸŸ¢ Normal\"))\n",
    "\n",
    "# conn.commit()\n",
    "# conn.close()\n",
    "\n",
    "# print(\"âœ… Tables created/updated with ingest_ts, and sample rows inserted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920732d2-0695-486c-a5da-3d6716bfc34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Downloading latest GeoLite2 database...\n",
      "ğŸ“¦ Extracting database...\n",
      "âœ… GeoLite2 database updated and ready at: ./resources/geoliteCountry/GeoLite2-Country.mmdb\n"
     ]
    }
   ],
   "source": [
    "# #setting up automated updation of ip vs cuntry geolite databse\n",
    "# import os\n",
    "# import tarfile\n",
    "# import requests\n",
    "\n",
    "# class GeoLite2Updater:\n",
    "#     def __init__(self, license_key, edition='GeoLite2-Country', extract_dir='./resources/geoliteCountry'):\n",
    "#         self.license_key = license_key\n",
    "#         self.edition = edition\n",
    "#         self.extract_dir = extract_dir\n",
    "#         self.download_path = f'{edition.lower()}.tar.gz'\n",
    "#         self.final_path = os.path.join(self.extract_dir, f'{edition}.mmdb')\n",
    "#         self.download_url = f'https://download.maxmind.com/app/geoip_download?edition_id={edition}&license_key={license_key}&suffix=tar.gz'\n",
    "\n",
    "#     def create_extract_dir(self):\n",
    "#         os.makedirs(self.extract_dir, exist_ok=True)\n",
    "\n",
    "#     def download_database(self):\n",
    "#         print(\"ğŸ“¥ Downloading latest GeoLite2 database...\")\n",
    "#         response = requests.get(self.download_url, stream=True)\n",
    "#         with open(self.download_path, 'wb') as f:\n",
    "#             for chunk in response.iter_content(chunk_size=8192):\n",
    "#                 f.write(chunk)\n",
    "\n",
    "#     def extract_database(self):\n",
    "#         print(\"ğŸ“¦ Extracting database...\")\n",
    "#         with tarfile.open(self.download_path, 'r:gz') as tar:\n",
    "#             for member in tar.getmembers():\n",
    "#                 if member.name.endswith('.mmdb'):\n",
    "#                     tar.extract(member, path=self.extract_dir)\n",
    "#                     extracted_path = os.path.join(self.extract_dir, member.name)\n",
    "#                     os.renames(extracted_path, self.final_path)\n",
    "\n",
    "#     def clean_up(self):\n",
    "#         if os.path.exists(self.download_path):\n",
    "#             os.remove(self.download_path)\n",
    "\n",
    "#     def update_database(self):\n",
    "#         self.create_extract_dir()\n",
    "#         self.download_database()\n",
    "#         self.extract_database()\n",
    "#         self.clean_up()\n",
    "#         print(f\"âœ… GeoLite2 database updated and ready at: {self.final_path}\")\n",
    "\n",
    "# #using class\n",
    "# LICENSE_KEY = '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'  # Replace with your real key\n",
    "# geo_updater = GeoLite2Updater(license_key=LICENSE_KEY)\n",
    "# geo_updater.update_database()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c83b3dd1-e45b-4629-83f0-6379f0e77f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import ipaddress\n",
    "import geoip2.database\n",
    "\n",
    "class GeoCountryUpdater:\n",
    "    \"\"\"\n",
    "    Fill in the `country` column for any log rows that still have\n",
    "    NULL or 'Unknown'.  Public IPs â†’ GeoLite2 lookup,\n",
    "    Private IPs â†’ 'Private/Local Network'.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 db_path: str = \"access_logs.db\",\n",
    "                 mmdb_path: str = \"./resources/geoliteCountry/GeoLite2-Country.mmdb\"):\n",
    "        self.db_path  = db_path\n",
    "        self.mmdb_path = mmdb_path\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    @staticmethod\n",
    "    def _is_private(ip: str) -> bool:\n",
    "        try:\n",
    "            return ipaddress.ip_address(ip.strip()).is_private\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    # ---------- workhorse ----------\n",
    "    def run(self):\n",
    "        conn   = sqlite3.connect(self.db_path)\n",
    "        cur    = conn.cursor()\n",
    "        reader = geoip2.database.Reader(self.mmdb_path)\n",
    "\n",
    "        # 1) make sure the column exists\n",
    "        try:\n",
    "            cur.execute(\"ALTER TABLE logs ADD COLUMN country TEXT\")\n",
    "        except sqlite3.OperationalError:\n",
    "            pass  # already there\n",
    "\n",
    "        # 2) grab ONLY rows that still need a value\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT rowid, ip\n",
    "            FROM   logs\n",
    "            WHERE  country IS NULL OR country = 'Unknown'\n",
    "        \"\"\")\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "        for rowid, ip in rows:\n",
    "            ip_clean = ip.strip()\n",
    "\n",
    "            if self._is_private(ip_clean):\n",
    "                country = \"Private/Local Network\"\n",
    "            else:\n",
    "                try:\n",
    "                    country = reader.country(ip_clean).country.name or \"Unknown\"\n",
    "                except Exception:\n",
    "                    country = \"Unknown\"\n",
    "\n",
    "            cur.execute(\"UPDATE logs SET country = ? WHERE rowid = ?\",\n",
    "                        (country, rowid))\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        reader.close()\n",
    "        print(f\"âœ… Enriched {len(rows)} rows that were Unknown/NULL.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd4f9e5a-2f9a-4973-9438-ecc5185b93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "# Advanced features saved to 'advanced_logs' table in access_log.db\n",
    "# TableName=advanced_logs; cols:ip,req_per_min,unique_urls,error_rate,avg_req_size_bytes,method_ratio_post_by_get,first_time_of_access\n",
    "\n",
    "\n",
    "class AdvancedLogFeatureBuilder:\n",
    "    def __init__(self, db_path='access_logs.db'):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self.df = None\n",
    "        self.final_df = None\n",
    "\n",
    "    def connect(self):\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load raw logs and prepare the dataframe safely.\"\"\"\n",
    "        self.df = pd.read_sql_query(\"SELECT * FROM logs\", self.conn)\n",
    "\n",
    "        # ip must be string\n",
    "        self.df['ip'] = self.df['ip'].astype(str)\n",
    "\n",
    "        # â”€â”€ ğŸ§¹ PRE-CLEAN the 'time' column â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        #  1) cast everything to str\n",
    "        self.df['time'] = self.df['time'].astype(str)\n",
    "\n",
    "        #  2) drop obviously bad entries (\"0\", \"\", None, etc.)\n",
    "        self.df = self.df[self.df['time'].str.len() > 5]\n",
    "\n",
    "        #  3) convert to datetime; invalid parses â†’ NaT (errors=\"coerce\")\n",
    "        self.df['time'] = pd.to_datetime(self.df['time'], errors='coerce')\n",
    "\n",
    "        #  4) drop any rows that still have NaT (optional but sensible)\n",
    "        self.df = self.df.dropna(subset=['time']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def feature_requests_per_minute(self):\n",
    "        self.df['minute'] = self.df['time'].dt.floor('T')\n",
    "        req_per_min= self.df.groupby(['ip', 'minute']).size().groupby('ip').mean().reset_index()\n",
    "        req_per_min.columns = ['ip', 'req_per_min']\n",
    "        return req_per_min\n",
    "\n",
    "    def feature_unique_urls(self):\n",
    "        unique_urls = self.df.groupby('ip')['url'].nunique().reset_index()\n",
    "        unique_urls.columns = ['ip', 'unique_urls']\n",
    "        return unique_urls\n",
    "\n",
    "    def feature_error_rate(self):\n",
    "        self.df['is_error'] = self.df['status'].astype(str).str.startswith(('4', '5'))\n",
    "        error_rate = self.df.groupby('ip')['is_error'].mean().reset_index()\n",
    "        error_rate.columns = ['ip', 'error_rate']\n",
    "        return error_rate\n",
    "\n",
    "    def feature_avg_req_size_bytes(self):\n",
    "        avg_req_size_bytes = self.df.groupby('ip')['size'].mean().reset_index()\n",
    "        avg_req_size_bytes.columns = ['ip', 'avg_req_size_bytes']\n",
    "        return avg_req_size_bytes\n",
    "\n",
    "    def feature_method_ratio_post_by_get(self):\n",
    "        methods = self.df[self.df['method'].isin(['GET', 'POST'])]\n",
    "        method_counts = methods.groupby(['ip', 'method']).size().unstack(fill_value=0)\n",
    "\n",
    "        if 'POST' not in method_counts.columns:\n",
    "            method_counts['POST'] = 0\n",
    "        if 'GET' not in method_counts.columns:\n",
    "            method_counts['GET'] = 0\n",
    "\n",
    "        method_counts['method_ratio_post_by_get'] = method_counts['POST'] / (method_counts['GET'] + 1e-6)\n",
    "        method_ratio_post_by_get = method_counts[['method_ratio_post_by_get']].reset_index()\n",
    "        return method_ratio_post_by_get\n",
    "\n",
    "    def feature_first_access_time(self):\n",
    "        first_time = self.df.groupby('ip')['time'].min().reset_index()\n",
    "        first_time.columns = ['ip', 'first_time_of_access']\n",
    "        return first_time\n",
    "\n",
    "    def merge_features(self):\n",
    "        features = [\n",
    "            self.feature_requests_per_minute(),\n",
    "            self.feature_unique_urls(),\n",
    "            self.feature_error_rate(),\n",
    "            self.feature_avg_req_size_bytes(),\n",
    "            self.feature_method_ratio_post_by_get(),\n",
    "            self.feature_first_access_time()\n",
    "        ]\n",
    "        self.final_df = reduce(lambda left, right: pd.merge(left, right, on='ip', how='outer'), features)\n",
    "        self.final_df = self.final_df.fillna(0)\n",
    "        # â”€â”€â”€ NEW: force datetime column â†’ plain ISO string â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if 'first_time_of_access' in self.final_df.columns:\n",
    "            self.final_df['first_time_of_access'] = (\n",
    "                self.final_df['first_time_of_access']\n",
    "                .astype(str)                              # â† cast fixes binding error\n",
    "            )\n",
    "        \n",
    "    def save_to_database(self, table_name='advanced_logs'):\n",
    "        self.final_df.to_sql(table_name, self.conn, if_exists='replace', index=False)\n",
    "        print(f\"âœ… Advanced features saved to '{table_name}' table.\")\n",
    "\n",
    "    def preview(self, limit=10):\n",
    "        preview_df = pd.read_sql_query(f\"SELECT * FROM advanced_logs LIMIT {limit}\", self.conn)\n",
    "        print(\"ğŸ“ Columns:\", list(preview_df.columns))\n",
    "        print(\"\\nğŸ“Š First 10 rows:\")\n",
    "        print(preview_df)\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "    def run(self, preview=False):\n",
    "        self.connect()\n",
    "        self.load_data()\n",
    "        self.merge_features()\n",
    "        self.save_to_database()\n",
    "        if preview:\n",
    "            self.preview()\n",
    "        self.close()\n",
    "\n",
    "#using class\n",
    "builder = AdvancedLogFeatureBuilder('access_logs.db')\n",
    "# builder.run(preview=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "db94e2ac-08d7-430a-995d-e31dd4eff36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 'ip_eachHour' with 24-hour hit distribution saved in access_logs.db itself\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "class HourlyHitAnalyzer:\n",
    "    def __init__(self, db_path='access_logs.db'):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self.df = None\n",
    "        self.pivot = None\n",
    "\n",
    "    def connect(self):\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "\n",
    "    def load_logs(self):\n",
    "        self.df = pd.read_sql_query(\"SELECT ip, time FROM logs\", self.conn)\n",
    "        self.df['time'] = pd.to_datetime(self.df['time'], errors='coerce')\n",
    "        self.df['hour'] = self.df['time'].dt.hour\n",
    "\n",
    "    def calculate_hits_per_hour(self):\n",
    "        hits = self.df.groupby(['ip', 'hour']).size().reset_index(name='hits')\n",
    "        self.pivot = hits.pivot(index='ip', columns='hour', values='hits').fillna(0).astype(int)\n",
    "\n",
    "    def rename_columns(self):\n",
    "        new_col_names = {i: f'{i}-{i+1}' for i in range(24)}\n",
    "        self.pivot.rename(columns=new_col_names, inplace=True)\n",
    "        self.pivot.reset_index(inplace=True)\n",
    "\n",
    "    def save_to_database(self, table_name='ip_eachHour'):\n",
    "        self.pivot.to_sql(table_name, self.conn, if_exists='replace', index=False)\n",
    "        print(f\"âœ… Table '{table_name}' with 24-hour hit distribution saved in {self.db_path}\")\n",
    "\n",
    "    def preview_table(self, table_name='ip_eachHour', limit=10):\n",
    "        df_preview = pd.read_sql_query(f\"SELECT * FROM {table_name} LIMIT {limit}\", self.conn)\n",
    "        print(f\"ğŸ“Š Sample from '{table_name}' table:\")\n",
    "        print(df_preview)\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "    def run_analysis(self, preview=False):\n",
    "        self.connect()\n",
    "        self.load_logs()\n",
    "        self.calculate_hits_per_hour()\n",
    "        self.rename_columns()\n",
    "        self.save_to_database()\n",
    "        if preview:\n",
    "            self.preview_table()\n",
    "        self.close()\n",
    "\n",
    "\n",
    "#using class\n",
    "analyzer = HourlyHitAnalyzer('access_logs.db')\n",
    "# analyzer.run_analysis(preview=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a0a90c54-d43d-436d-a17a-09df40d33ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#created new table ip_eachHour_category with cols: ip,hour,category\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "class EachHourCategoryClassifier:\n",
    "    def __init__(self, db_path='access_logs.db'):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self.df = None\n",
    "        self.result_df = None\n",
    "\n",
    "    def connect(self):\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df = pd.read_sql_query(\"SELECT * FROM ip_eachHour\", self.conn)\n",
    "\n",
    "    def classify(self, hits):\n",
    "        if hits <= 1:\n",
    "            return 'Idle / Minimal'\n",
    "        elif hits <= 50:\n",
    "            return 'Casual Human'\n",
    "        elif hits <= 200:\n",
    "            return 'Active Human'\n",
    "        elif hits <= 1000:\n",
    "            return 'Automation / Crawler'\n",
    "        elif hits <= 3000:\n",
    "            return 'Aggressive Bot'\n",
    "        elif hits <= 5000:\n",
    "            return 'Credential Stuffing / Vulnerability Scans'\n",
    "        elif hits <= 10000:\n",
    "            return 'DoS Behavior'\n",
    "        else:\n",
    "            return 'DoS Botnet / Amplification'\n",
    "\n",
    "    def melt_and_classify(self):\n",
    "        melted_df = self.df.melt(id_vars='ip', var_name='hour', value_name='hits')\n",
    "        melted_df['category'] = melted_df['hits'].apply(self.classify)\n",
    "        self.result_df = melted_df[['ip', 'hour', 'category']]\n",
    "\n",
    "    def save_to_db(self, table_name='ip_eachHour_category'):\n",
    "        self.result_df.to_sql(table_name, self.conn, if_exists='replace', index=False)\n",
    "        print(f\"âœ… Hour-wise category table saved to '{table_name}'.\")\n",
    "\n",
    "    def preview(self, limit=48):\n",
    "        preview_df = pd.read_sql_query(\"SELECT * FROM ip_eachHour_category LIMIT ?\", self.conn, params=(limit,))\n",
    "        print(\"ğŸ“Š Sample from 'ip_eachHour_category':\")\n",
    "        print(preview_df)\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "    def run(self, preview=False):\n",
    "        self.connect()\n",
    "        self.load_data()\n",
    "        self.melt_and_classify()\n",
    "        self.save_to_db()\n",
    "        if preview:\n",
    "            self.preview()\n",
    "        self.close()\n",
    "\n",
    "#using class\n",
    "classifier = EachHourCategoryClassifier('access_logs.db')\n",
    "# classifier.run(preview=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb82eac5-e939-4cc0-981a-0f4b47a84b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ddos_multiple_ip ready\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Create a table to log every *multi-IP-per-second* swarm incident\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import sqlite3\n",
    "\n",
    "def init_ddos_table(db_path=\"access_logs.db\"):\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        conn.executescript(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ddos_multiple_ip (\n",
    "            id            INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            -- ISO-8601 timestamps mark the exact second bucket\n",
    "            window_start  TEXT,\n",
    "            window_end    TEXT,\n",
    "            duration_s    INTEGER,       -- currently always 1 s\n",
    "            total_hits    INTEGER,       -- #requests seen in that second\n",
    "            unique_ips    INTEGER,       -- how many different IPs joined\n",
    "            peak_rps      INTEGER,       -- same as total_hits for 1-s window\n",
    "            inserted_at   TEXT DEFAULT (datetime('now'))\n",
    "        );\n",
    "        \"\"\")\n",
    "    print(\"âœ… ddos_multiple_ip ready\")\n",
    "\n",
    "# call it once\n",
    "init_ddos_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b224f13f-d435-41f5-8c02-9c4ac302fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Real-time multi-IP per-second burst detector\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3\n",
    "\n",
    "class MultiIPDDoSDetector:\n",
    "    \"\"\"\n",
    "    Watches a sliding 1-second window of raw log events (ip, ts).\n",
    "    When the window exceeds BOTH:\n",
    "        â€¢ hits_thr  â€“ total requests\n",
    "        â€¢ uniq_thr  â€“ distinct IPs\n",
    "    â†’ (1) write an incident row to ddos_multiple_ip\n",
    "      (2) call alert_callback(set_of_ips, incident_dict)\n",
    "    A cooldown stops duplicate alerts during the same attack.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_path: str,\n",
    "        alert_callback,            # function that handles swarm IPs\n",
    "        window_s:   int = 1,       # analysis bucket size (seconds)\n",
    "        hits_thr:   int = 800,     # tweak for your traffic baseline\n",
    "        uniq_thr:   int = 120,     #    \"\n",
    "        cooldown_s: int = 60       # min seconds between incidents\n",
    "    ):\n",
    "        # config\n",
    "        self.db_path    = db_path\n",
    "        self.alert_cb   = alert_callback\n",
    "        self.window_s   = window_s\n",
    "        self.hits_thr   = hits_thr\n",
    "        self.uniq_thr   = uniq_thr\n",
    "        self.cooldown_s = cooldown_s\n",
    "        # runtime state\n",
    "        self.events        = deque()      # stores (timestamp, ip)\n",
    "        self.last_alert_ts = None         # last incident time\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # ingest() MUST be called for **every** raw log line you receive\n",
    "    # ------------------------------------------------------------------\n",
    "    def ingest(self, ip: str, ts: datetime):\n",
    "        self.events.append((ts, ip))\n",
    "\n",
    "        # drop events older than our sliding window\n",
    "        cutoff = ts - timedelta(seconds=self.window_s)\n",
    "        while self.events and self.events[0][0] < cutoff:\n",
    "            self.events.popleft()\n",
    "\n",
    "        # after updating the window â†’ check thresholds\n",
    "        self._maybe_fire(ts)\n",
    "\n",
    "    # ------------------------------------ internal helpers --------\n",
    "    def _maybe_fire(self, now: datetime):\n",
    "        total_hits = len(self.events)\n",
    "        uniq_ips   = len({ip for _, ip in self.events})\n",
    "\n",
    "        # skip if thresholds not met\n",
    "        if total_hits < self.hits_thr or uniq_ips < self.uniq_thr:\n",
    "            return\n",
    "\n",
    "        # skip if still in cooldown\n",
    "        if self.last_alert_ts and (now - self.last_alert_ts).total_seconds() < self.cooldown_s:\n",
    "            return\n",
    "\n",
    "        # build incident summary\n",
    "        incident = dict(\n",
    "            window_start = (now - timedelta(seconds=self.window_s)).isoformat(),\n",
    "            window_end   = now.isoformat(),\n",
    "            duration_s   = self.window_s,\n",
    "            total_hits   = total_hits,\n",
    "            unique_ips   = uniq_ips,\n",
    "            peak_rps     = total_hits            # = total_hits for 1-s window\n",
    "        )\n",
    "        # 1ï¸âƒ£ store in SQLite\n",
    "        self._write_incident(incident)\n",
    "        # 2ï¸âƒ£ remember last fire time\n",
    "        self.last_alert_ts = now\n",
    "        # 3ï¸âƒ£ collect participating IPs\n",
    "        swarm_ips = {ip for _, ip in self.events}\n",
    "        print(f\"ğŸŒŠ DDoS burst logged! hits={total_hits} uniq={uniq_ips}\")\n",
    "        # 4ï¸âƒ£ hand off to the pipeline (adds to ip_suspicious + alerts)\n",
    "        self.alert_cb(swarm_ips, incident)\n",
    "\n",
    "    def _write_incident(self, row: dict):\n",
    "        \"\"\"Insert incident row into ddos_multiple_ip.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\"\"\"\n",
    "                INSERT INTO ddos_multiple_ip\n",
    "                (window_start, window_end, duration_s,\n",
    "                 total_hits, unique_ips, peak_rps)\n",
    "                VALUES (:window_start,:window_end,:duration_s,\n",
    "                        :total_hits,:unique_ips,:peak_rps)\n",
    "            \"\"\", row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "97688d8f-01c7-49da-b353-d7b3a5a7b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #configuration\n",
    "# Updated Config class with Together.ai API\n",
    "class Config:\n",
    "    DB_PATH = \"access_logs.db\"\n",
    "    OPENAI_API_KEY = \"6d5f9d8edb25a1743e5272f75f52a818ead6a95635e57b122118fb82d754c697\"\n",
    "    SLACK_WEBHOOK_URL = \"https://hooks.slack.com/services/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\"\n",
    "    SMTP_SERVER = \"smtp.gmail.com\"\n",
    "    SMTP_PORT = 587\n",
    "    EMAIL_SENDER = \"aashij971@gmail.com\"\n",
    "    EMAIL_PASSWORD = \"bbff hzuj lczj bhmy\"\n",
    "    EMAIL_RECEIVER = \"aashijainbid@gmail.com\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bab3e180-0364-412d-8dc5-e712a6945e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "class SuspiciousIPDetector:\n",
    "    def __init__(self, db_path):\n",
    "        self.conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "\n",
    "    def get_suspicious_ips(self):\n",
    "        # Thresholds\n",
    "        MIN_req_per_min= 10\n",
    "        MIN_UNIQUE_URLS = 15\n",
    "        MAX_ERROR_RATE = 0.2\n",
    "        MAX_avg_req_size_bytes = 10000\n",
    "        MAX_method_ratio_post_by_get = 3.0\n",
    "        SCORE_THRESHOLD = 5  # Minimum weighted score to consider suspicious\n",
    "\n",
    "        # High-risk behavior categories\n",
    "        suspicious_categories = [\n",
    "            'ğŸŸ  Automation / Crawlers', 'ğŸ”´ Aggressive Bot',\n",
    "            'ğŸ”´ Credential Stuffing', 'ğŸ”´ Vulnerability Scans',\n",
    "            'ğŸ”´ DoS Behavior', 'ğŸš¨ DoS Botnets / Amplification'\n",
    "        ]\n",
    "\n",
    "        # ğŸ§  Category-based IPs\n",
    "        category_query = f\"\"\"\n",
    "            SELECT DISTINCT ip FROM ip_eachHour_category\n",
    "            WHERE category IN ({','.join(['?']*len(suspicious_categories))})\n",
    "        \"\"\"\n",
    "        category_df = pd.read_sql_query(category_query, self.conn, params=suspicious_categories)\n",
    "\n",
    "        # ğŸ§  Threshold + weighted score logic\n",
    "        adv_df = pd.read_sql_query(\"SELECT * FROM advanced_logs\", self.conn)\n",
    "        if adv_df.empty:\n",
    "            return category_df['ip'].tolist()  # return only category-based if advanced is empty\n",
    "\n",
    "        # adv_df['hour'] = pd.to_datetime(adv_df['first_time_of_access']).dt.hour\n",
    "        # Parse strings â†’ pandas datetimes\n",
    "        adv_df['first_time_of_access'] = pd.to_datetime(adv_df['first_time_of_access'])\n",
    "\n",
    "        # Safely ensure every stamp is Asia/Kolkata\n",
    "        adv_df['first_time_of_access'] = adv_df['first_time_of_access'].apply(\n",
    "                lambda ts: ts.tz_localize('Asia/Kolkata')        # naive â†’ attach IST\n",
    "               if ts.tzinfo is None                  # already aware â†’ convert\n",
    "               else ts.tz_convert('Asia/Kolkata')\n",
    "        )\n",
    "\n",
    "        # Now extract the local hour\n",
    "        adv_df['hour'] = adv_df['first_time_of_access'].dt.hour\n",
    "\n",
    "\n",
    "        is_odd_hour = adv_df['hour'] % 2 != 0\n",
    "\n",
    "        # Assign weighted score per condition\n",
    "        adv_df['score'] = 0\n",
    "        adv_df.loc[adv_df['req_per_min'] > MIN_req_per_min, 'score'] += 1.5\n",
    "        adv_df.loc[adv_df['unique_urls'] > MIN_UNIQUE_URLS, 'score'] += 1.5\n",
    "        adv_df.loc[adv_df['error_rate'] > MAX_ERROR_RATE, 'score'] += 2\n",
    "        adv_df.loc[adv_df['avg_req_size_bytes'] > MAX_avg_req_size_bytes, 'score'] += 1\n",
    "        adv_df.loc[adv_df['method_ratio_post_by_get'] > MAX_method_ratio_post_by_get, 'score'] += 2\n",
    "        adv_df.loc[is_odd_hour, 'score'] += 1\n",
    "\n",
    "        threshold_ips = adv_df[adv_df['score'] >= SCORE_THRESHOLD]['ip'].tolist()\n",
    "        category_ips = category_df['ip'].tolist()\n",
    "\n",
    "        return list(set(threshold_ips + category_ips))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dec793bf-c5a2-49af-abd3-05614ac3d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml based (isolation forest) suspicious ip detector\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class MLBasedAnomalyDetector:\n",
    "    def __init__(self, db_path):\n",
    "        self.conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "\n",
    "    def get_features(self):\n",
    "        df = pd.read_sql_query(\"SELECT * FROM advanced_logs\", self.conn)\n",
    "        X = df[[\"req_per_min\", \"unique_urls\", \"error_rate\", \"avg_req_size_bytes\", \"method_ratio_post_by_get\"]]\n",
    "        return df[\"ip\"], X\n",
    "\n",
    "    def detect_anomalies(self):\n",
    "        ips, X = self.get_features()\n",
    "        if X.empty:\n",
    "            return []                     # â¬… early-return, avoids IsolationForest crash\n",
    "        model = IsolationForest(contamination=0.05, random_state=42)\n",
    "        preds = model.fit_predict(X)\n",
    "        return ips[preds == -1].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0b5cba8-b868-4ab0-964b-5d306c04e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress   \n",
    "class IPContextFetcher:\n",
    "    def __init__(self, db_path):\n",
    "        self.conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "\n",
    "    def _is_private(self, ip: str) -> bool:\n",
    "        try:\n",
    "            return ipaddress.ip_address(ip).is_private\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def get_ip_context(self, ip):\n",
    "        \"\"\"\n",
    "        Return a context dict for the given IP.\n",
    "        Never returns None â€“ at minimum {'ip': ip}.\n",
    "        \"\"\"\n",
    "        context = {\"ip\": ip}\n",
    "\n",
    "        # â”€â”€ 1) advanced_logs  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        adv_df = pd.read_sql_query(\n",
    "            \"SELECT * FROM advanced_logs WHERE ip = ?\",\n",
    "            self.conn, params=[ip]\n",
    "        )\n",
    "        if not adv_df.empty:\n",
    "            adv = adv_df.iloc[0].to_dict()\n",
    "            context.update({\n",
    "                \"req_per_min\":            adv.get(\"req_per_min\", \"n/a\"),\n",
    "                \"error_rate (4xx+5xx)/total req\": adv.get(\"error_rate\", \"n/a\"),\n",
    "                \"unique_urls\":            adv.get(\"unique_urls\", \"n/a\"),\n",
    "                \"avg_req_size_bytes\":     adv.get(\"avg_req_size_bytes\", \"n/a\"),\n",
    "                \"method_ratio_post_by_get\": adv.get(\"method_ratio_post_by_get\", \"n/a\"),\n",
    "                \"first_time_of_access\":   adv.get(\"first_time_of_access\", \"n/a\"),\n",
    "            })\n",
    "        else:\n",
    "            # fill missing numeric fields with \"n/a\"\n",
    "            context.update({\n",
    "                \"req_per_min\": \"n/a\",\n",
    "                \"error_rate (4xx+5xx)/total req\": \"n/a\",\n",
    "                \"unique_urls\": \"n/a\",\n",
    "                \"avg_req_size_bytes\": \"n/a\",\n",
    "                \"method_ratio_post_by_get\": \"n/a\",\n",
    "                \"first_time_of_access\": \"n/a\",\n",
    "            })\n",
    "\n",
    "        # â”€â”€ 2) Top 5 URLs  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        urls_df = pd.read_sql_query(\n",
    "            \"SELECT url FROM logs WHERE ip = ?\", self.conn, params=[ip]\n",
    "        )\n",
    "        context[\"top_5_urls\"] = (\n",
    "            urls_df[\"url\"].value_counts().head(5).index.tolist()\n",
    "            if not urls_df.empty else []\n",
    "        )\n",
    "\n",
    "        # â”€â”€ 3) Hourly categories  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        hourly_df = pd.read_sql_query(\n",
    "            \"SELECT hour, category FROM ip_eachHour_category WHERE ip = ?\",\n",
    "            self.conn, params=[ip]\n",
    "        )\n",
    "        context[\"categories_by_hour\"] = hourly_df.to_dict(orient=\"records\")\n",
    "\n",
    "        # â”€â”€ 4) Country  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        country_df = pd.read_sql_query(\"\"\"\n",
    "            SELECT country\n",
    "            FROM logs\n",
    "            WHERE ip = ?\n",
    "              AND country IS NOT NULL\n",
    "              AND country <> ''\n",
    "            LIMIT 1;\n",
    "        \"\"\", self.conn, params=[ip])\n",
    "\n",
    "        if not country_df.empty:\n",
    "            context[\"country\"] = country_df[\"country\"].iloc[0]\n",
    "        else:\n",
    "            context[\"country\"] = (\n",
    "                \"Private/Local Network\" if self._is_private(ip) else \"Unknown\"\n",
    "            )\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b264b0b3-489a-4b9d-a420-7a95e24b7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #propmt generator and gen ai interface\n",
    "# Updated GenAIExplainer using Together.ai for LLaMA models\n",
    "import requests\n",
    "\n",
    "class GenAIExplainer:\n",
    "    def __init__(self, api_key, model_name=\"meta-llama/Llama-3-70b-chat-hf\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.api_url = \"https://api.together.xyz/v1/chat/completions\"\n",
    "\n",
    "    def generate_prompt(self, data):\n",
    "        hourly_summary = \"\\n\".join([f\"Hour {row['hour']}: {row['category']}\" for row in data['categories_by_hour']])\n",
    "        return f\"\"\"\n",
    "    IP: {data['ip']}\n",
    "    Country: {data['country']}\n",
    "    Request Rate: {data['req_per_min']} req/min\n",
    "    Error Rate: {data['error_rate']}\n",
    "    Unique URLs: {data['unique_urls']}\n",
    "    Avg Request Size: {data['avg_req_size_bytes']}\n",
    "    Method Ratio Post/Get: {data['method_ratio_post_by_get']}\n",
    "    First Access Time: {data['first_time']}\n",
    "    Top URLs: {', '.join(data['top_5_urls'])}\n",
    "    Hourly Categories:\\n{hourly_summary}\n",
    "    Explain this behavior.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def get_explanation(self, prompt):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a cybersecurity analyst.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 500\n",
    "        }\n",
    "        response = requests.post(self.api_url, headers=headers, json=payload)\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c497e446-a56e-4c37-9a8a-3c03085c8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert sender (email+slack)\n",
    "import smtplib\n",
    "import requests\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart \n",
    "\n",
    "class AlertSender:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def send_email(self, subject, body):\n",
    "        try:\n",
    "            msg = MIMEText(body)\n",
    "            msg['Subject'] = subject\n",
    "            msg['From'] = self.config.EMAIL_SENDER\n",
    "            msg['To'] = self.config.EMAIL_RECEIVER\n",
    "            with smtplib.SMTP(self.config.SMTP_SERVER, self.config.SMTP_PORT) as server:\n",
    "                server.starttls()\n",
    "                server.login(self.config.EMAIL_SENDER, self.config.EMAIL_PASSWORD)\n",
    "                server.sendmail(self.config.EMAIL_SENDER, [self.config.EMAIL_RECEIVER], msg.as_string())\n",
    "            print(\"âœ… Email alert sent\")\n",
    "        except Exception as e:\n",
    "            \n",
    "            print(f\"âŒ Email error: {e}\")\n",
    "\n",
    "    def send_slack_alert(self, message):\n",
    "        try:\n",
    "            resp = requests.post(self.config.SLACK_WEBHOOK_URL, json={\"text\": message})\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"âŒ Slack error: {resp.status_code} - {resp.text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Slack error: {e}\")\n",
    "\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # NEW  â–¸ send a single digest for an entire DDoS burst\n",
    "    # ----------------------------------------------------------------\n",
    "    def send_burst(self, incident: dict, ip_list: list[str]):\n",
    "        \"\"\"\n",
    "        incident = {'window_start', 'window_end', 'total_hits',\n",
    "                    'unique_ips', 'duration_s', 'peak_rps', â€¦}\n",
    "        ip_list  = list of all participating IPs (already sorted)\n",
    "        rps means request per sec\n",
    "        \"\"\"\n",
    "        subject = (f\"ğŸš¨ DDoS burst: {incident['unique_ips']} IPs, \"\n",
    "                   f\"{incident['total_hits']} reqs in {incident['duration_s']} s\")\n",
    "\n",
    "        body = (\n",
    "            f\"ğŸŒŠ DDoS detected {incident['window_start']} â†’ {incident['window_end']}\\n\"\n",
    "            f\"â€¢ Total hits : {incident['total_hits']}\\n\"\n",
    "            f\"â€¢ Unique IPs : {incident['unique_ips']}\\n\"\n",
    "            f\"â€¢ Peak RPS   : {incident['peak_rps']}\\n\\n\"\n",
    "            \"Top offender IPs (first 30):\\n\"\n",
    "            + \"\\n\".join(f\"  â€¢ {ip}\" for ip in ip_list[:30])\n",
    "            + (\"\\nâ€¦ (truncated)\" if len(ip_list) > 30 else \"\")\n",
    "        )\n",
    "\n",
    "        # one SMTP login + one Slack POST\n",
    "        self.send_email(subject, body)\n",
    "        self.send_slack_alert(f\"{subject}\\n{body}\")\n",
    "\n",
    "\n",
    "    def send(self, ip, context, explanation):\n",
    "        \"\"\"\n",
    "        Build the e-mail / Slack message for a suspicious IP.\n",
    "        â€¢ Works even if the context dict is minimal (e.g. only {\"ip\": ip}).\n",
    "        \"\"\"\n",
    "        subject = f\"ğŸš¨ Suspicious IP Detected: {ip}\"\n",
    "\n",
    "        # 1ï¸âƒ£  All context fields except the bulky hour table\n",
    "        context_text = \"\\n\".join(\n",
    "            f\"{k}: {v}\" for k, v in context.items() if k != \"categories_by_hour\"\n",
    "        )\n",
    "\n",
    "        # 2ï¸âƒ£  Hourly categories â—€ safe fallback when key is missing\n",
    "        cat_rows = context.get(\"categories_by_hour\", [])   # â† returns [] if key absent\n",
    "        category_text = \"\\n\".join(\n",
    "            f\"  Hour {row['hour']} â¤ {row['category']}\" for row in cat_rows\n",
    "        ) or \"n/a\"\n",
    "\n",
    "        # 3ï¸âƒ£  Final body\n",
    "        body = f\"\"\"ğŸ“Œ CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "â± Hourly Categories:\n",
    "{category_text}\n",
    "\n",
    "ğŸ§  EXPLANATION:\n",
    "{explanation}\"\"\"\n",
    "\n",
    "        # 4ï¸âƒ£  Send alerts\n",
    "        self.send_email(subject, body)\n",
    "        self.send_slack_alert(f\"{subject}\\n{body}\")\n",
    "\n",
    "        # 5ï¸âƒ£  Debug print\n",
    "        print(\"ğŸ“¤ Preparing to send alert:\")\n",
    "        print(\"Context:\", context)\n",
    "        print(\"Explanation:\", explanation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed4a0f5a-4cb3-4f41-890b-bbde8e0bc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo \n",
    "import pandas as pd\n",
    "from collections import defaultdict          # NEW âœ perâ€‘IP locking\n",
    "\n",
    "_ip_locks = defaultdict(threading.Lock)      # NEW âœ one Lock object per unique IP\n",
    "\n",
    "class AnomalyExplainerPipeline:\n",
    "    IST = ZoneInfo(\"Asia/Kolkata\")\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.detector = SuspiciousIPDetector(config.DB_PATH)\n",
    "        self.ml_detector = MLBasedAnomalyDetector(config.DB_PATH)\n",
    "        self.fetcher = IPContextFetcher(config.DB_PATH)\n",
    "        self.alert = AlertSender(config)\n",
    "        # âš™ï¸ Prepare GenAI explainer for future LLaMA integration (disabled in test mode)\n",
    "        # self.genai = GenAIExplainer(api_key=config.LLAMA_API_KEY)\n",
    "\n",
    "\n",
    "    def is_ip_previously_flagged(self, ip):\n",
    "        conn = sqlite3.connect(self.config.DB_PATH, check_same_thread=False)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT 1 FROM ip_suspicious WHERE suspiciousIp = ?\", (ip,))\n",
    "        result = cursor.fetchone()\n",
    "        conn.close()\n",
    "        return result is not None\n",
    "\n",
    "\n",
    "    def block_ip(self, ip: str, *, detected_at: str | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Block an IP and record:\n",
    "        â€¢ detected_at = arrival-time (in IST) when the detector fired\n",
    "        â€¢ backend_blocked_at  = block moment (in IST) when block_ip called\n",
    "        If detected_at is None we look up the newest logs.ingest_ts for that IP.\n",
    "        \"\"\"\n",
    "\n",
    "        conn   = sqlite3.connect(self.config.DB_PATH, check_same_thread=False)\n",
    "        cur    = conn.cursor()\n",
    "\n",
    "        # fallback: newest ingest_ts for this IP\n",
    "        if detected_at is None:\n",
    "            cur.execute(\n",
    "            \"SELECT ingest_ts FROM logs WHERE ip=? ORDER BY ingest_ts DESC LIMIT 1\",\n",
    "            (ip,)\n",
    "            )\n",
    "            row          = cur.fetchone()\n",
    "            detected_at  = row[0] if row else datetime.now(self.IST).isoformat()\n",
    "\n",
    "        backend_blocked_at = datetime.now(self.IST).isoformat()  # LOCAL (IST)\n",
    "\n",
    "        print(f\"ğŸš« Blocking {ip} | detected_at={detected_at} | backend_locked_at={backend_blocked_at}\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS blocked_log (\n",
    "            ip TEXT PRIMARY KEY,\n",
    "            detected_at TEXT,\n",
    "            backend_blocked_at  TEXT,\n",
    "            detection_count INTEGER DEFAULT 1\n",
    "        )\n",
    "        \"\"\")\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO blocked_log (ip, detected_at, backend_blocked_at, detection_count)\n",
    "            VALUES (?, ?, ?, 1)\n",
    "            ON CONFLICT(ip) DO UPDATE\n",
    "                SET backend_blocked_at      = excluded.backend_blocked_at,\n",
    "                detection_count = detection_count + 1\n",
    "        \"\"\", (ip, detected_at, backend_blocked_at))\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "    def insert_suspicious_ip(self, ip: str, forced_reason: str | None = None):\n",
    "        conn = sqlite3.connect(self.config.DB_PATH, check_same_thread=False)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # table may not exist on first run â†’ create lazily\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS ip_suspicious (\n",
    "                suspiciousIp   TEXT PRIMARY KEY,\n",
    "                time           TEXT,\n",
    "                reason         TEXT,\n",
    "                detection_count INTEGER DEFAULT 0\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # Decide the \"reason\" field\n",
    "        if forced_reason:\n",
    "            reason = forced_reason                       # e.g. \"DDoS burst\"\n",
    "        else:\n",
    "            # find most common behaviour category for that IP\n",
    "            category_df = pd.read_sql_query(\"\"\"\n",
    "                SELECT category, COUNT(*) AS cnt\n",
    "                FROM   ip_eachHour_category\n",
    "                WHERE  ip = ?\n",
    "                GROUP BY category\n",
    "                ORDER BY cnt DESC\n",
    "                LIMIT 1;\n",
    "            \"\"\", conn, params=[ip])\n",
    "            top_cat = category_df['category'][0] if not category_df.empty else \"Unknown Category\"\n",
    "\n",
    "            # figure out which engine(s) flagged the IP\n",
    "            rule_ips = set(self.detector.get_suspicious_ips())\n",
    "            ml_ips   = set(self.ml_detector.detect_anomalies())\n",
    "            if ip in rule_ips and ip in ml_ips:\n",
    "                src = \"via Rule + ML\"\n",
    "            elif ip in rule_ips:\n",
    "                src = \"via Rule\"\n",
    "            elif ip in ml_ips:\n",
    "                src = \"via ML\"\n",
    "            else:\n",
    "                src = \"via Unknown\"\n",
    "\n",
    "            reason = f\"{top_cat} ({src})\"\n",
    "\n",
    "        now = datetime.now(self.IST).isoformat()   # e.g. 2025-06-27T18:25:00+05:30\n",
    "\n",
    "        # INSERT if new, otherwise UPDATE timestamp & increment counter\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT OR IGNORE INTO ip_suspicious (suspiciousIp, time, reason, detection_count)\n",
    "            VALUES (?, ?, ?, 0);\n",
    "        \"\"\", (ip, now, reason))\n",
    "        cursor.execute(\"\"\"\n",
    "            UPDATE ip_suspicious\n",
    "            SET time = ?, reason = ?, detection_count = detection_count + 1\n",
    "            WHERE suspiciousIp = ?;\n",
    "        \"\"\", (now, reason, ip))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print(f\"âœ… ip_suspicious â‡¢ {ip} â€¢ {reason}\")\n",
    "\n",
    "    \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # DDoS burst callback  â†’  ONE digest alert, then per-IP DB/block\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def handle_ddos_ips(self, ip_set: set[str], incident: dict):\n",
    "        ip_list = sorted(ip_set)\n",
    "\n",
    "        # 1ï¸âƒ£  ONE summary e-mail / Slack\n",
    "        self.alert.send_burst(incident, ip_list)\n",
    "\n",
    "        # 2ï¸âƒ£  Still record and block each IP, but NO per-IP e-mails\n",
    "        for ip in ip_list:\n",
    "            self.insert_suspicious_ip(ip, forced_reason=\"DDoS burst\")\n",
    "            # 5ï¸âƒ£ simulate block\n",
    "            self.block_ip(ip)\n",
    "\n",
    "\n",
    "    def _last_alert_time(self, ip: str):\n",
    "        \"\"\"\n",
    "        Return the last time this IP was inserted into ip_suspicious,\n",
    "        always as a timezone-aware datetime in Asia/Kolkata.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.config.DB_PATH, check_same_thread=False)\n",
    "        row  = conn.execute(\n",
    "            \"SELECT time FROM ip_suspicious WHERE suspiciousIp = ?\",\n",
    "            (ip,)\n",
    "        ).fetchone()\n",
    "        conn.close()\n",
    "\n",
    "        if not row:\n",
    "            return None                                   # never alerted\n",
    "\n",
    "        ts_str = row[0]\n",
    "\n",
    "        # Convert string â†’ datetime.  If the stored string already has\n",
    "        # â€œ+05:30â€ it comes out zone-aware; otherwise we pin it to IST.\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(ts_str)\n",
    "            if dt.tzinfo is None:                        # naive â†’ attach IST\n",
    "                dt = dt.replace(tzinfo=self.IST)\n",
    "        except Exception:                                # malformed â†’ use now\n",
    "            dt = datetime.now(self.IST)\n",
    "\n",
    "        return dt\n",
    "\n",
    "    def process_single_ip(self, ip):\n",
    "        # ğŸ”’ perâ€‘IP mutex\n",
    "        lock = _ip_locks[ip]\n",
    "        with lock:\n",
    "\n",
    "            # skip if we alerted in the last 30â€¯min\n",
    "            last_alert = self._last_alert_time(ip)\n",
    "            if last_alert and (datetime.now(self.IST) - last_alert).total_seconds() < 1800:\n",
    "                return\n",
    "\n",
    "            # run detectors\n",
    "            rule_ips = set(self.detector.get_suspicious_ips())\n",
    "            ml_ips   = set(self.ml_detector.detect_anomalies())\n",
    "\n",
    "            if ip in rule_ips or ip in ml_ips:\n",
    "                now_iso = datetime.now(self.IST).isoformat()\n",
    "\n",
    "                # decide context: full on first sighting, minimal on repeats\n",
    "                context = ({\"info\": \"Previously flagged\"} \n",
    "                           if last_alert else\n",
    "                           self.fetcher.get_ip_context(ip))\n",
    "\n",
    "                # record / refresh\n",
    "                self.insert_suspicious_ip(ip)\n",
    "                self.block_ip(ip, detected_at=now_iso)\n",
    "\n",
    "                # async alert\n",
    "                threading.Thread(\n",
    "                    target=self._async_alert,\n",
    "                    args=(ip, context, rule_ips, ml_ips),\n",
    "                    daemon=True\n",
    "                ).start()\n",
    "\n",
    "\n",
    "    def _async_alert(self, ip, context, rule_ips, ml_ips):\n",
    "         # âœ… Generate explanation\n",
    "        # ğŸ’¬ LLaMA GenAI response (future): Replace static explanation with LLaMA output\n",
    "        # prompt = self.alert.genai.generate_prompt(context)  # â¬…ï¸ Uncomment if using GenAI\n",
    "        # explanation = self.alert.genai.get_explanation(prompt)  # â¬…ï¸ Will fetch LLaMA-powered analysis\n",
    "        \n",
    "        explanation = (\n",
    "            f\"Rule-based: {'Yes' if ip in rule_ips else 'No'}, \"\n",
    "            f\"ML-based: {'Yes' if ip in ml_ips else 'No'} \"\n",
    "            \"ğŸ“Œ Note: GenAI skipped\"\n",
    "        )\n",
    "        self.alert.send(ip, context, explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e01008f-f40a-487d-a2a0-e87f98548c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime, timedelta          # â† keep\n",
    "from zoneinfo import ZoneInfo                     # ğŸ’  NEW\n",
    "import time, sqlite3, pandas as pd\n",
    "\n",
    "IST = ZoneInfo(\"Asia/Kolkata\")                    # ğŸ’  NEW\n",
    "\n",
    "def simulate_realtime_stream(pipeline, interval: int = 1):\n",
    "    \"\"\"\n",
    "    Wakes every `interval` seconds:\n",
    "      â€¢ Feeds the last 2-second slice of logs into the DDoS detector\n",
    "      â€¢ Runs rule+ML on previously unseen IPs\n",
    "    \"\"\"\n",
    "    global seen_ips\n",
    "    seen_ips = set()\n",
    "    print(\"ğŸš€ Real-time suspicious-IP & DDoS monitoring started\")\n",
    "\n",
    "    ddos_watcher = MultiIPDDoSDetector(\n",
    "        pipeline.config.DB_PATH,\n",
    "        alert_callback=pipeline.handle_ddos_ips\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # 1ï¸âƒ£ â”€â”€ Pull last 2-second burst (using ingest_ts) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            now_ist        = datetime.now(IST)\n",
    "            window_start   = (now_ist - timedelta(seconds=2)).isoformat()\n",
    "\n",
    "            with sqlite3.connect(pipeline.config.DB_PATH) as conn:\n",
    "                recent = pd.read_sql_query(\n",
    "                    \"SELECT ip, ingest_ts AS ts \"\n",
    "                    \"FROM   logs \"\n",
    "                    \"WHERE  ingest_ts >= ?\",\n",
    "                    conn, params=(window_start,)\n",
    "                )\n",
    "\n",
    "            # feed each row to the 1-sec burst detector\n",
    "            for _, row in recent.iterrows():\n",
    "                try:\n",
    "                    ts = datetime.fromisoformat(row['ts'])     # ğŸ’  string â†’ datetime\n",
    "                except Exception:\n",
    "                    ts = datetime.now(IST)                     # fallback\n",
    "                ddos_watcher.ingest(ip=row['ip'], ts=ts)\n",
    "\n",
    "            # 2ï¸âƒ£ â”€â”€ Rule+ML for brand-new IPs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            conn = sqlite3.connect(pipeline.config.DB_PATH)\n",
    "            all_ips_df = pd.read_sql_query(\"SELECT DISTINCT ip FROM logs\", conn)\n",
    "            conn.close()\n",
    "\n",
    "            new_ips = [ip for ip in all_ips_df['ip'] if ip not in seen_ips]\n",
    "            for ip in new_ips:\n",
    "                print(f\"\\nğŸ“¡ [New IP Detected] {ip}\")\n",
    "                pipeline.process_single_ip(ip)\n",
    "                seen_ips.add(ip)\n",
    "\n",
    "            time.sleep(interval)                                 # 3ï¸âƒ£ wait\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in real-time loop: {e}\")\n",
    "            time.sleep(interval)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbf7dd6a-60e8-4474-af27-d658f33e3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To use:\n",
    "# pipeline = AnomalyExplainerPipeline(Config())\n",
    "# simulate_one_batch(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5db664-2dd7-4244-9f90-074455d20859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Table: logs\n",
      "ğŸ”¸ Columns: ['id', 'ip', 'time', 'method', 'url', 'status', 'size', 'agent', 'country', 'ingest_ts']\n",
      "{'id': 1, 'ip': '1.202.218.8', 'time': '2012-06-20T22:35:12+05:30', 'method': 'GET', 'url': '/robots.txt', 'status': 404, 'size': 492, 'agent': '\"\\\\\"Mozilla/5.0\"', 'country': 'China', 'ingest_ts': '2025-07-04T00:02:49.256095+05:30'}\n",
      "{'id': 2, 'ip': '208.115.113.91', 'time': '2012-06-20T22:50:16+05:30', 'method': 'GET', 'url': '/logs/?C=M;O=D', 'status': 200, 'size': 1278, 'agent': 'Mozilla/5.0 (compatible; Ezooms/1.0; ezooms.bot@gmail.com)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:02:50.724848+05:30'}\n",
      "{'id': 3, 'ip': '123.125.71.20', 'time': '2012-06-20T23:00:40+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 912, 'agent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'country': 'China', 'ingest_ts': '2025-07-04T00:02:52.275504+05:30'}\n",
      "{'id': 4, 'ip': '220.181.108.101', 'time': '2012-06-20T23:01:01+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 912, 'agent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'country': 'China', 'ingest_ts': '2025-07-04T00:02:53.766115+05:30'}\n",
      "{'id': 5, 'ip': '123.125.68.79', 'time': '2012-06-20T23:23:24+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 625, 'agent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'country': 'China', 'ingest_ts': '2025-07-04T00:02:55.348056+05:30'}\n",
      "{'id': 6, 'ip': '178.154.210.252', 'time': '2012-06-20T23:24:10+05:30', 'method': 'GET', 'url': '/?C=S;O=A', 'status': 200, 'size': 663, 'agent': 'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)', 'country': 'Russia', 'ingest_ts': '2025-07-04T00:02:56.929608+05:30'}\n",
      "{'id': 7, 'ip': '74.125.126.102', 'time': '2012-06-20T23:45:28+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 606, 'agent': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022; InfoPath.2; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:02:58.536073+05:30'}\n",
      "{'id': 8, 'ip': '74.125.126.103', 'time': '2012-06-20T23:45:29+05:30', 'method': 'GET', 'url': '/icons/blank.gif', 'status': 200, 'size': 383, 'agent': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022; InfoPath.2; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:00.131501+05:30'}\n",
      "{'id': 9, 'ip': '74.125.126.93', 'time': '2012-06-20T23:45:29+05:30', 'method': 'GET', 'url': '/icons/folder.gif', 'status': 200, 'size': 460, 'agent': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022; InfoPath.2; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:01.805967+05:30'}\n",
      "{'id': 10, 'ip': '74.125.126.82', 'time': '2012-06-20T23:45:30+05:30', 'method': 'GET', 'url': '/favicon.ico', 'status': 404, 'size': 449, 'agent': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022; InfoPath.2; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:03.406428+05:30'}\n",
      "{'id': 11, 'ip': '184.82.92.239', 'time': '2012-06-21T00:33:44+05:30', 'method': 'GET', 'url': '/logs/access.log', 'status': 200, 'size': 2519, 'agent': 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; .NET CLR 3.5.30729)', 'country': 'Thailand', 'ingest_ts': '2025-07-04T00:03:04.996362+05:30'}\n",
      "{'id': 12, 'ip': '173.236.21.106', 'time': '2012-06-21T00:46:22+05:30', 'method': 'GET', 'url': '/robots.txt', 'status': 404, 'size': 488, 'agent': 'Mozilla/5.0 (compatible; MJ12bot/v1.4.3; http://www.majestic12.co.uk/bot.php?+)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:07.271712+05:30'}\n",
      "{'id': 13, 'ip': '173.236.21.106', 'time': '2012-06-21T00:46:23+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 621, 'agent': 'Mozilla/5.0 (compatible; MJ12bot/v1.4.3; http://www.majestic12.co.uk/bot.php?+)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:09.663572+05:30'}\n",
      "{'id': 14, 'ip': '213.186.122.2', 'time': '2012-06-21T00:57:53+05:30', 'method': 'GET', 'url': '/logs/?C=D;O=D', 'status': 200, 'size': 658, 'agent': 'Mozilla/5.0 (compatible; AhrefsBot/3.0; +http://ahrefs.com/robot/)', 'country': 'Russia', 'ingest_ts': '2025-07-04T00:03:11.957812+05:30'}\n",
      "{'id': 15, 'ip': '66.249.72.65', 'time': '2012-06-21T00:58:00+05:30', 'method': 'GET', 'url': '/robots.txt', 'status': 404, 'size': 508, 'agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:14.305468+05:30'}\n",
      "{'id': 16, 'ip': '66.249.72.65', 'time': '2012-06-21T00:58:00+05:30', 'method': 'GET', 'url': '/logs/', 'status': 200, 'size': 723, 'agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:16.732231+05:30'}\n",
      "{'id': 17, 'ip': '123.125.71.44', 'time': '2012-06-21T01:08:57+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 913, 'agent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'country': 'China', 'ingest_ts': '2025-07-04T00:03:19.034049+05:30'}\n",
      "{'id': 18, 'ip': '220.181.108.88', 'time': '2012-06-21T01:09:48+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 913, 'agent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'country': 'China', 'ingest_ts': '2025-07-04T00:03:21.392312+05:30'}\n",
      "{'id': 19, 'ip': '178.154.210.252', 'time': '2012-06-21T01:15:12+05:30', 'method': 'GET', 'url': '/logs/', 'status': 200, 'size': 728, 'agent': 'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)', 'country': 'Russia', 'ingest_ts': '2025-07-04T00:03:23.760878+05:30'}\n",
      "{'id': 20, 'ip': '139.18.2.209', 'time': '2012-06-21T02:01:43+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 912, 'agent': 'findlinks/2.6 (+http://wortschatz.uni-leipzig.de/findlinks/)', 'country': 'Germany', 'ingest_ts': '2025-07-04T00:03:26.071962+05:30'}\n",
      "{'id': 21, 'ip': '123.125.71.48', 'time': '2012-06-21T02:08:14+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 913, 'agent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'country': 'China', 'ingest_ts': '2025-07-04T00:03:28.524974+05:30'}\n",
      "{'id': 22, 'ip': '220.181.108.95', 'time': '2012-06-21T02:09:03+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 913, 'agent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'country': 'China', 'ingest_ts': '2025-07-04T00:03:30.897874+05:30'}\n",
      "{'id': 23, 'ip': '123.125.67.218', 'time': '2012-06-21T02:11:50+05:30', 'method': 'GET', 'url': '/robots.txt', 'status': 404, 'size': 465, 'agent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'country': 'China', 'ingest_ts': '2025-07-04T00:03:33.333064+05:30'}\n",
      "{'id': 24, 'ip': '180.76.6.224', 'time': '2012-06-21T02:37:26+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 908, 'agent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'country': 'China', 'ingest_ts': '2025-07-04T00:03:35.765800+05:30'}\n",
      "{'id': 25, 'ip': '208.115.113.91', 'time': '2012-06-21T02:43:59+05:30', 'method': 'GET', 'url': '/robots.txt', 'status': 404, 'size': 469, 'agent': 'Mozilla/5.0 (compatible; Ezooms/1.0; ezooms.bot@gmail.com)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:38.184043+05:30'}\n",
      "{'id': 26, 'ip': '208.115.113.91', 'time': '2012-06-21T03:24:06+05:30', 'method': 'GET', 'url': '/logs/?C=M;O=D', 'status': 200, 'size': 1278, 'agent': 'Mozilla/5.0 (compatible; Ezooms/1.0; ezooms.bot@gmail.com)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:40.437809+05:30'}\n",
      "{'id': 27, 'ip': '123.125.71.57', 'time': '2012-06-21T03:54:49+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 908, 'agent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'country': 'China', 'ingest_ts': '2025-07-04T00:03:42.801627+05:30'}\n",
      "{'id': 28, 'ip': '220.181.108.96', 'time': '2012-06-21T03:54:53+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 908, 'agent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'country': 'China', 'ingest_ts': '2025-07-04T00:03:45.534736+05:30'}\n",
      "{'id': 29, 'ip': '1.202.218.8', 'time': '2012-06-21T04:05:33+05:30', 'method': 'GET', 'url': '/robots.txt', 'status': 404, 'size': 488, 'agent': '\"\\\\\"Mozilla/5.0\"', 'country': 'China', 'ingest_ts': '2025-07-04T00:03:48.328926+05:30'}\n",
      "{'id': 30, 'ip': '220.181.108.149', 'time': '2012-06-21T04:11:48+05:30', 'method': 'GET', 'url': '/icons/folder.gif', 'status': 200, 'size': 479, 'agent': 'Baiduspider-image+(+http://www.baidu.com/search/spider.htm)', 'country': 'China', 'ingest_ts': '2025-07-04T00:03:50.900106+05:30'}\n",
      "{'id': 31, 'ip': '173.192.34.95', 'time': '2012-06-21T04:14:32+05:30', 'method': 'GET', 'url': '/robots.txt', 'status': 404, 'size': 530, 'agent': 'Aboundex/0.2 (http://www.aboundex.com/crawler/)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:53.622022+05:30'}\n",
      "{'id': 32, 'ip': '173.192.34.95', 'time': '2012-06-21T04:14:32+05:30', 'method': 'GET', 'url': '/', 'status': 200, 'size': 949, 'agent': 'Aboundex/0.2 (http://www.aboundex.com/crawler/)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:56.272754+05:30'}\n",
      "{'id': 33, 'ip': '173.192.34.95', 'time': '2012-06-21T04:14:32+05:30', 'method': 'GET', 'url': '/?C=N;O=D', 'status': 200, 'size': 949, 'agent': 'Aboundex/0.2 (http://www.aboundex.com/crawler/)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:03:58.910808+05:30'}\n",
      "{'id': 34, 'ip': '173.192.34.95', 'time': '2012-06-21T04:14:32+05:30', 'method': 'GET', 'url': '/?C=M;O=A', 'status': 200, 'size': 949, 'agent': 'Aboundex/0.2 (http://www.aboundex.com/crawler/)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:04:01.587783+05:30'}\n",
      "{'id': 35, 'ip': '173.192.34.95', 'time': '2012-06-21T04:14:32+05:30', 'method': 'GET', 'url': '/?C=S;O=A', 'status': 200, 'size': 949, 'agent': 'Aboundex/0.2 (http://www.aboundex.com/crawler/)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:04:04.174975+05:30'}\n",
      "{'id': 36, 'ip': '173.192.34.95', 'time': '2012-06-21T04:14:32+05:30', 'method': 'GET', 'url': '/?C=D;O=A', 'status': 200, 'size': 949, 'agent': 'Aboundex/0.2 (http://www.aboundex.com/crawler/)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:04:06.779309+05:30'}\n",
      "{'id': 37, 'ip': '173.192.34.95', 'time': '2012-06-21T04:14:32+05:30', 'method': 'GET', 'url': '/logs/', 'status': 200, 'size': 1338, 'agent': 'Aboundex/0.2 (http://www.aboundex.com/crawler/)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:04:09.376657+05:30'}\n",
      "{'id': 38, 'ip': '173.192.34.95', 'time': '2012-06-21T04:14:32+05:30', 'method': 'GET', 'url': '/?C=N;O=A', 'status': 200, 'size': 949, 'agent': 'Aboundex/0.2 (http://www.aboundex.com/crawler/)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:04:12.013608+05:30'}\n",
      "{'id': 39, 'ip': '173.192.34.95', 'time': '2012-06-21T04:14:33+05:30', 'method': 'GET', 'url': '/?C=M;O=D', 'status': 200, 'size': 949, 'agent': 'Aboundex/0.2 (http://www.aboundex.com/crawler/)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:04:14.674789+05:30'}\n",
      "{'id': 40, 'ip': '173.192.34.95', 'time': '2012-06-21T04:14:33+05:30', 'method': 'GET', 'url': '/?C=S;O=D', 'status': 200, 'size': 949, 'agent': 'Aboundex/0.2 (http://www.aboundex.com/crawler/)', 'country': 'United States', 'ingest_ts': '2025-07-04T00:04:17.300424+05:30'}\n",
      "\n",
      "ğŸ“‚ Table: sqlite_sequence\n",
      "ğŸ”¸ Columns: ['name', 'seq']\n",
      "{'name': 'logs', 'seq': 40}\n",
      "\n",
      "ğŸ“‚ Table: ip_suspicious\n",
      "ğŸ”¸ Columns: ['suspiciousIp', 'time', 'reason', 'detection_count']\n",
      "{'suspiciousIp': '184.82.92.239', 'time': '2025-07-04T00:03:05.409880+05:30', 'reason': 'Idle / Minimal (via ML)', 'detection_count': 1}\n",
      "\n",
      "ğŸ“‚ Table: ddos_multiple_ip\n",
      "ğŸ”¸ Columns: ['id', 'window_start', 'window_end', 'duration_s', 'total_hits', 'unique_ips', 'peak_rps', 'inserted_at']\n",
      "âš ï¸ No data in this table.\n",
      "\n",
      "ğŸ“‚ Table: blocked_log\n",
      "ğŸ”¸ Columns: ['ip', 'detected_at', 'backend_blocked_at', 'detection_count', 'client_blocked_at', 'client_block_status']\n",
      "{'ip': '184.82.92.239', 'detected_at': '2025-07-04T00:03:05.307501+05:30', 'backend_blocked_at': '2025-07-04T00:03:05.413998+05:30', 'detection_count': 1, 'client_blocked_at': '2025-07-04T00:03:07.268433+05:30', 'client_block_status': 'failed'}\n",
      "\n",
      "ğŸ“‚ Table: advanced_logs\n",
      "ğŸ”¸ Columns: ['ip', 'req_per_min', 'unique_urls', 'error_rate', 'avg_req_size_bytes', 'method_ratio_post_by_get', 'first_time_of_access']\n",
      "{'ip': '1.202.218.8', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 1.0, 'avg_req_size_bytes': 490.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-20 22:35:12+05:30'}\n",
      "{'ip': '123.125.67.218', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 1.0, 'avg_req_size_bytes': 465.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 02:11:50+05:30'}\n",
      "{'ip': '123.125.68.79', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 625.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-20 23:23:24+05:30'}\n",
      "{'ip': '123.125.71.20', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 912.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-20 23:00:40+05:30'}\n",
      "{'ip': '123.125.71.44', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 913.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 01:08:57+05:30'}\n",
      "{'ip': '123.125.71.48', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 913.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 02:08:14+05:30'}\n",
      "{'ip': '123.125.71.57', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 908.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 03:54:49+05:30'}\n",
      "{'ip': '139.18.2.209', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 912.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 02:01:43+05:30'}\n",
      "{'ip': '173.192.34.95', 'req_per_min': 10.0, 'unique_urls': 10, 'error_rate': 0.1, 'avg_req_size_bytes': 946.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 04:14:32+05:30'}\n",
      "{'ip': '173.236.21.106', 'req_per_min': 2.0, 'unique_urls': 2, 'error_rate': 0.5, 'avg_req_size_bytes': 554.5, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 00:46:22+05:30'}\n",
      "{'ip': '178.154.210.252', 'req_per_min': 1.0, 'unique_urls': 2, 'error_rate': 0.0, 'avg_req_size_bytes': 695.5, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-20 23:24:10+05:30'}\n",
      "{'ip': '180.76.6.224', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 908.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 02:37:26+05:30'}\n",
      "{'ip': '184.82.92.239', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 2519.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 00:33:44+05:30'}\n",
      "{'ip': '208.115.113.91', 'req_per_min': 1.0, 'unique_urls': 2, 'error_rate': 0.3333333333333333, 'avg_req_size_bytes': 1008.3333333333334, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-20 22:50:16+05:30'}\n",
      "{'ip': '213.186.122.2', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 658.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 00:57:53+05:30'}\n",
      "{'ip': '220.181.108.101', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 912.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-20 23:01:01+05:30'}\n",
      "{'ip': '220.181.108.149', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 479.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 04:11:48+05:30'}\n",
      "{'ip': '220.181.108.88', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 913.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 01:09:48+05:30'}\n",
      "{'ip': '220.181.108.95', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 913.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 02:09:03+05:30'}\n",
      "{'ip': '220.181.108.96', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 908.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 03:54:53+05:30'}\n",
      "{'ip': '66.249.72.65', 'req_per_min': 2.0, 'unique_urls': 2, 'error_rate': 0.5, 'avg_req_size_bytes': 615.5, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-21 00:58:00+05:30'}\n",
      "{'ip': '74.125.126.102', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 606.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-20 23:45:28+05:30'}\n",
      "{'ip': '74.125.126.103', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 383.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-20 23:45:29+05:30'}\n",
      "{'ip': '74.125.126.82', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 1.0, 'avg_req_size_bytes': 449.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-20 23:45:30+05:30'}\n",
      "{'ip': '74.125.126.93', 'req_per_min': 1.0, 'unique_urls': 1, 'error_rate': 0.0, 'avg_req_size_bytes': 460.0, 'method_ratio_post_by_get': 0.0, 'first_time_of_access': '2012-06-20 23:45:29+05:30'}\n",
      "\n",
      "ğŸ“‚ Table: ip_eachHour\n",
      "ğŸ”¸ Columns: ['ip', '0-1', '1-2', '2-3', '3-4', '4-5', '22-23', '23-24']\n",
      "{'ip': '1.202.218.8', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 1, '22-23': 1, '23-24': 0}\n",
      "{'ip': '123.125.67.218', '0-1': 0, '1-2': 0, '2-3': 1, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '123.125.68.79', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 1}\n",
      "{'ip': '123.125.71.20', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 1}\n",
      "{'ip': '123.125.71.44', '0-1': 0, '1-2': 1, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '123.125.71.48', '0-1': 0, '1-2': 0, '2-3': 1, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '123.125.71.57', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 1, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '139.18.2.209', '0-1': 0, '1-2': 0, '2-3': 1, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '173.192.34.95', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 10, '22-23': 0, '23-24': 0}\n",
      "{'ip': '173.236.21.106', '0-1': 2, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '178.154.210.252', '0-1': 0, '1-2': 1, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 1}\n",
      "{'ip': '180.76.6.224', '0-1': 0, '1-2': 0, '2-3': 1, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '184.82.92.239', '0-1': 1, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '208.115.113.91', '0-1': 0, '1-2': 0, '2-3': 1, '3-4': 1, '4-5': 0, '22-23': 1, '23-24': 0}\n",
      "{'ip': '213.186.122.2', '0-1': 1, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '220.181.108.101', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 1}\n",
      "{'ip': '220.181.108.149', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 1, '22-23': 0, '23-24': 0}\n",
      "{'ip': '220.181.108.88', '0-1': 0, '1-2': 1, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '220.181.108.95', '0-1': 0, '1-2': 0, '2-3': 1, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '220.181.108.96', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 1, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '66.249.72.65', '0-1': 2, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 0}\n",
      "{'ip': '74.125.126.102', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 1}\n",
      "{'ip': '74.125.126.103', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 1}\n",
      "{'ip': '74.125.126.82', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 1}\n",
      "{'ip': '74.125.126.93', '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0, '22-23': 0, '23-24': 1}\n",
      "\n",
      "ğŸ“‚ Table: ip_eachHour_category\n",
      "ğŸ”¸ Columns: ['ip', 'hour', 'category']\n",
      "{'ip': '1.202.218.8', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.67.218', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.68.79', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.20', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.44', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.48', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.57', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '139.18.2.209', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.192.34.95', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.236.21.106', 'hour': '0-1', 'category': 'Casual Human'}\n",
      "{'ip': '178.154.210.252', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '180.76.6.224', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '184.82.92.239', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '208.115.113.91', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '213.186.122.2', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.101', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.149', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.88', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.95', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.96', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '66.249.72.65', 'hour': '0-1', 'category': 'Casual Human'}\n",
      "{'ip': '74.125.126.102', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.103', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.82', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.93', 'hour': '0-1', 'category': 'Idle / Minimal'}\n",
      "{'ip': '1.202.218.8', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.67.218', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.68.79', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.20', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.44', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.48', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.57', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '139.18.2.209', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.192.34.95', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.236.21.106', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '178.154.210.252', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '180.76.6.224', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '184.82.92.239', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '208.115.113.91', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '213.186.122.2', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.101', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.149', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.88', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.95', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.96', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '66.249.72.65', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.102', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.103', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.82', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.93', 'hour': '1-2', 'category': 'Idle / Minimal'}\n",
      "{'ip': '1.202.218.8', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.67.218', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.68.79', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.20', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.44', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.48', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.57', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '139.18.2.209', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.192.34.95', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.236.21.106', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '178.154.210.252', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '180.76.6.224', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '184.82.92.239', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '208.115.113.91', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '213.186.122.2', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.101', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.149', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.88', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.95', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.96', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '66.249.72.65', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.102', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.103', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.82', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.93', 'hour': '2-3', 'category': 'Idle / Minimal'}\n",
      "{'ip': '1.202.218.8', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.67.218', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.68.79', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.20', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.44', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.48', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.57', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '139.18.2.209', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.192.34.95', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.236.21.106', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '178.154.210.252', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '180.76.6.224', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '184.82.92.239', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '208.115.113.91', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '213.186.122.2', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.101', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.149', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.88', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.95', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.96', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '66.249.72.65', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.102', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.103', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.82', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.93', 'hour': '3-4', 'category': 'Idle / Minimal'}\n",
      "{'ip': '1.202.218.8', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.67.218', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.68.79', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.20', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.44', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.48', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.57', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '139.18.2.209', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.192.34.95', 'hour': '4-5', 'category': 'Casual Human'}\n",
      "{'ip': '173.236.21.106', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '178.154.210.252', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '180.76.6.224', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '184.82.92.239', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '208.115.113.91', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '213.186.122.2', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.101', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.149', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.88', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.95', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.96', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '66.249.72.65', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.102', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.103', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.82', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.93', 'hour': '4-5', 'category': 'Idle / Minimal'}\n",
      "{'ip': '1.202.218.8', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.67.218', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.68.79', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.20', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.44', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.48', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.57', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '139.18.2.209', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.192.34.95', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.236.21.106', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '178.154.210.252', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '180.76.6.224', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '184.82.92.239', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '208.115.113.91', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '213.186.122.2', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.101', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.149', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.88', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.95', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.96', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '66.249.72.65', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.102', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.103', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.82', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.93', 'hour': '22-23', 'category': 'Idle / Minimal'}\n",
      "{'ip': '1.202.218.8', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.67.218', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.68.79', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.20', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.44', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.48', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '123.125.71.57', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '139.18.2.209', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.192.34.95', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '173.236.21.106', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '178.154.210.252', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '180.76.6.224', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '184.82.92.239', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '208.115.113.91', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '213.186.122.2', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.101', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.149', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.88', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.95', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '220.181.108.96', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '66.249.72.65', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.102', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.103', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.82', 'hour': '23-24', 'category': 'Idle / Minimal'}\n",
      "{'ip': '74.125.126.93', 'hour': '23-24', 'category': 'Idle / Minimal'}\n"
     ]
    }
   ],
   "source": [
    "#pinting access_logs.db to verify\n",
    "import sqlite3\n",
    "\n",
    "def print_all_tables(db_path):\n",
    "    conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch all table names\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    for (table_name,) in tables:\n",
    "        print(f\"\\nğŸ“‚ Table: {table_name}\")\n",
    "        \n",
    "        # Get column names\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        columns = [col[1] for col in cursor.fetchall()]\n",
    "        print(f\"ğŸ”¸ Columns: {columns}\")\n",
    "        \n",
    "        # Fetch and print all rowsx\n",
    "        cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "        rows = cursor.fetchall()\n",
    "        if rows:\n",
    "            for row in rows:\n",
    "                print(dict(zip(columns, row)))\n",
    "        else:\n",
    "            print(\"âš ï¸ No data in this table.\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "# Use the function\n",
    "print_all_tables(\"access_logs.db\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7004c496-1937-40c0-ad70-2f3c24ee8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "\n",
    "# def clear_all_tables(db_path=\"access_logs.db\"):\n",
    "#     conn = sqlite3.connect(db_path)\n",
    "#     cursor = conn.cursor()\n",
    "\n",
    "#     # Get list of all table names\n",
    "#     cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "#     tables = cursor.fetchall()\n",
    "\n",
    "#     for (table_name,) in tables:\n",
    "#         print(f\"ğŸ§¹ Clearing table: {table_name}\")\n",
    "#         cursor.execute(f\"DELETE FROM {table_name}\")\n",
    "\n",
    "#     conn.commit()\n",
    "#     conn.close()\n",
    "#     print(\"âœ… All tables cleared successfully.\")\n",
    "\n",
    "# clear_all_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df5ac95-482c-44e4-8629-23aac7dcb86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running â†’ plt_suspects_last15_days_bars â€¦ No suspiciousâ€‘IP data in the last 15â€¯days.\n",
      "done âœ“\n",
      "Running â†’ plt_platform_pie_latest_day â€¦ logs table empty.\n",
      "done âœ“\n",
      "Running â†’ plt_status_pie_latest_day â€¦ logs table empty.\n",
      "done âœ“\n",
      "Running â†’ plt_top_urls_latest_day â€¦ logs table empty.\n",
      "done âœ“\n",
      "Running â†’ plt_country_req_latest_day â€¦ logs table empty.\n",
      "done âœ“\n",
      "Running â†’ plt_suspicious_countries_last30d â€¦ logs table empty.\n",
      "done âœ“\n",
      "Running â†’ export_category_breakdown_latest_day_all_hours â€¦ âŒ  No day with category data found.\n",
      "done âœ“\n",
      "Running â†’ plt_avg_size_trend_latest_day â€¦ logs table empty.\n",
      "done âœ“\n",
      "Running â†’ plt_detection_counts_last7d â€¦ No suspicion events between 2025-06-27 and 2025-07-03.\n",
      "done âœ“\n",
      "Running â†’ plt_heatmap â€¦ done âœ“\n",
      "Running â†’ plt_browser_top10_latest_day â€¦ logs table empty.\n",
      "done âœ“\n",
      "Running â†’ plt_size_vs_status_latest_day â€¦ logs table empty.\n",
      "done âœ“\n",
      "Running â†’ plt_blocked_ips_latest_day â€¦ blocked_log empty\n",
      "done âœ“\n",
      "Running â†’ plt_request_methods â€¦ done âœ“\n",
      "Running â†’ plt_suspicious_reasons â€¦ FAILED âœ—  (fromisoformat: argument must be str)\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Blueâ€¯Guard â€“Â Plots (Jupyter / .py friendly)\n",
    "#  Updated: fixes trendâ€‘line crash & browser bar chart\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from ipywidgets import interact, widgets\n",
    "import re\n",
    "\n",
    "# â”€â”€ Paths & global style â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_DIR   = pathlib.Path().absolute()\n",
    "STATIC_DIR = BASE_DIR / \"static\"\n",
    "STATIC_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\")\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"font.size\"]      = 12\n",
    "\n",
    "# â”€â”€ DB helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _run_sql(db_path: str, query: str, **kw) -> pd.DataFrame:\n",
    "    return pd.read_sql_query(query, sqlite3.connect(db_path), **kw)\n",
    "\n",
    "# â”€â”€ small helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _fill_24h(df, col=\"cnt\", default=0):\n",
    "    base = pd.DataFrame({\"hr\": range(24)})\n",
    "    return base.merge(df, on=\"hr\", how=\"left\").fillna({col: default})\n",
    "\n",
    "def _split_agent(ua: str):\n",
    "    ua = (ua or \"\").lower()\n",
    "    # platform\n",
    "    if   \"android\" in ua: plat = \"Android\"\n",
    "    elif any(k in ua for k in [\"iphone\",\"ipad\",\"ios\"]): plat = \"iOS\"\n",
    "    elif \"windows\" in ua: plat = \"Windows\"\n",
    "    elif \"mac os x\" in ua: plat = \"macOS\"\n",
    "    elif \"linux\"   in ua: plat = \"Linux\"\n",
    "    else:                    plat = \"Other\"\n",
    "    # browser\n",
    "    if   \"edge\" in ua:                    br = \"Edge\"\n",
    "    elif \"chrome\"  in ua and \"chromium\" not in ua: br = \"Chrome\"\n",
    "    elif \"safari\"  in ua and \"chrome\"   not in ua: br = \"Safari\"\n",
    "    elif \"firefox\" in ua:                br = \"Firefox\"\n",
    "    elif any(k in ua for k in [\"curl\",\"wget\"]):     br = \"CLI\"\n",
    "    else: br = \"Other\"\n",
    "    return plat, br\n",
    "\n",
    "def _save_plot(fname):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(STATIC_DIR / fname, dpi=120, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  PLOT FUNCTIONS  (only updated ones shown in full;\n",
    "#  untouched ones remain the same as before)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
    "\n",
    "def plt_avg_size_trend_latest_day(db,\n",
    "                                  *,\n",
    "                                  save=True,\n",
    "                                  prefix=\"avg_size_day\",\n",
    "                                  static_subdir=\"\"):\n",
    "    \"\"\"\n",
    "    Plot average responseâ€‘payload size (bytes) per hour\n",
    "    for the latest eventâ€‘day in `logs.time`.\n",
    "    File is named   <prefix>_<YYYYâ€‘MMâ€‘DD>.png\n",
    "    \"\"\"\n",
    "    # â”€â”€ 1. find the latest event day â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    latest_day_row = _run_sql(db, \"SELECT DATE(MAX(time)) AS d FROM logs\")\n",
    "    if latest_day_row.empty or latest_day_row.iloc[0,0] is None:\n",
    "        print(\"logs table empty.\"); return\n",
    "    day = latest_day_row.iloc[0,0]            # e.g. '2025-07-03'\n",
    "\n",
    "    # â”€â”€ 2. average size per hour for that day â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df = _run_sql(db, f\"\"\"\n",
    "        SELECT\n",
    "            CAST(strftime('%H', time) AS INT) AS hr,\n",
    "            AVG(CAST(size AS REAL))           AS sz\n",
    "        FROM   logs\n",
    "        WHERE  DATE(time) = '{day}'\n",
    "          AND  size IS NOT NULL\n",
    "        GROUP  BY hr\n",
    "    \"\"\")\n",
    "    if df.empty:\n",
    "        print(f\"No size data for {day}.\"); return\n",
    "\n",
    "    df = _fill_24h(df, col=\"sz\").sort_values(\"hr\")\n",
    "    df[\"label\"] = (\n",
    "        df[\"hr\"].astype(str).str.zfill(2) + \"-\" +\n",
    "        ((df[\"hr\"] + 1) % 24).astype(str).str.zfill(2)\n",
    "    )\n",
    "\n",
    "    # â”€â”€ 3. plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    ax = sns.lineplot(data=df, x=\"label\", y=\"sz\",\n",
    "                      marker=\"o\", linewidth=2)\n",
    "\n",
    "    if df[\"sz\"].notna().any():\n",
    "        for idx, color in [(df[\"sz\"].idxmax(), \"red\"),\n",
    "                           (df[\"sz\"].idxmin(), \"green\")]:\n",
    "            ax.scatter(df.loc[idx, \"label\"], df.loc[idx, \"sz\"],\n",
    "                       color=color, s=120, zorder=5)\n",
    "\n",
    "    ax.set(\n",
    "        title=f\"Average Payload Size by Hour  ({day})\\n\"\n",
    "              \"(Redâ€¯=\\u00A0Max,â€¯Greenâ€¯=\\u00A0Min)\",\n",
    "        xlabel=\"Hour window\",\n",
    "        ylabel=\"Bytes\"\n",
    "    )\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    # â”€â”€ 4. save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if save:\n",
    "        fname = f\"{prefix}_{day}.png\"\n",
    "        if static_subdir:\n",
    "            fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "        _save_plot(fname)\n",
    "        print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  HTTPâ€‘status donut â€¢ latest day only\n",
    "# ------------------------------------------------------------------\n",
    "def plt_status_pie_latest_day(db):\n",
    "    \"\"\"Plot status code distribution (latest day only) as a donut chart with legend.\"\"\"\n",
    "    # Step 1: Get latest day\n",
    "    day_row = _run_sql(db, \"SELECT DATE(MAX(time)) AS d FROM logs\")\n",
    "    if day_row.empty or day_row.iloc[0, 0] is None:\n",
    "        print(\"logs table empty.\"); return\n",
    "    day = day_row.iloc[0, 0]\n",
    "\n",
    "    # Step 2: Query status code classes for latest day\n",
    "    df = _run_sql(db, f\"\"\"\n",
    "        SELECT CASE\n",
    "            WHEN status BETWEEN 200 AND 299 THEN '2xx Success'\n",
    "            WHEN status BETWEEN 300 AND 399 THEN '3xx Redirect'\n",
    "            WHEN status BETWEEN 400 AND 499 THEN '4xx Client Error'\n",
    "            WHEN status BETWEEN 500 AND 599 THEN '5xx Server Error'\n",
    "            ELSE 'Other' END AS status_class,\n",
    "            COUNT(*) cnt\n",
    "        FROM logs\n",
    "        WHERE DATE(time) = '{day}'\n",
    "        GROUP BY status_class\n",
    "    \"\"\")\n",
    "    if df.empty:\n",
    "        print(f\"No status data for {day}.\"); return\n",
    "\n",
    "    # Step 3: Plot donut chart\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    colors = sns.color_palette('viridis', len(df))\n",
    "    total = df[\"cnt\"].sum()\n",
    "\n",
    "    wedges, _ = plt.pie(\n",
    "        df[\"cnt\"],\n",
    "        startangle=90,\n",
    "        colors=colors,\n",
    "        wedgeprops=dict(width=0.4, edgecolor='w'),\n",
    "        labels=None  # <-- hide labels in pie\n",
    "    )\n",
    "\n",
    "    # Step 4: Legend with full details\n",
    "    legend_labels = [\n",
    "        f\"{row.status_class} â€“ {row.cnt/total*100:0.1f}% ({row.cnt:,})\"\n",
    "        for row in df.itertuples()\n",
    "    ]\n",
    "    plt.legend(wedges, legend_labels,\n",
    "               title=\"Status Class\",\n",
    "               loc=\"center left\",\n",
    "               bbox_to_anchor=(1.0, 0.5),\n",
    "               frameon=True)\n",
    "\n",
    "    plt.title(f\"HTTP Status Code Distribution â€“ {day}\", weight=\"bold\", pad=20)\n",
    "\n",
    "    # Step 5: Save\n",
    "    fname = f\"status_pie_{day}.png\"\n",
    "    _save_plot(fname)\n",
    "    print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "#  Pie / donut of ALL platforms on the latest day  â€“ keeps â€œOtherâ€\n",
    "# -----------------------------------------------------------------\n",
    "def plt_platform_pie_latest_day(db_path: str,\n",
    "                                *,\n",
    "                                save: bool = True,\n",
    "                                prefix: str = \"platform_pie_day\",\n",
    "                                static_subdir: str = \"\"):\n",
    "    \"\"\"\n",
    "    Pie chart of platform share for the mostâ€‘recent calendar day\n",
    "    in `logs.time`, using _split_agent() to classify UA strings.\n",
    "\n",
    "    â€¢ â€œOtherâ€ is kept as its own slice when present.\n",
    "    â€¢ Legend shows  <Platform â€“ x.xâ€¯% (hits)>  with colours matching slices.\n",
    "    â€¢ Saved to  static/<static_subdir>/<prefix>_<YYYYâ€‘MMâ€‘DD>_<UTCts>.png\n",
    "    \"\"\"\n",
    "    # 1ï¸âƒ£  detect latest day in logs\n",
    "    day_row = _run_sql(db_path,\n",
    "        \"SELECT DATE(MAX(time)) AS d FROM logs\")\n",
    "    if day_row.empty or day_row.iloc[0, 0] is None:\n",
    "        print(\"logs table empty.\"); return\n",
    "    day = day_row.iloc[0, 0]                          # e.g. '2025-07-03'\n",
    "\n",
    "    # 2ï¸âƒ£  pull userâ€‘agents for that day (cap rows if huge)\n",
    "    df_raw = _run_sql(db_path, f\"\"\"\n",
    "        SELECT agent\n",
    "        FROM   logs\n",
    "        WHERE  DATE(time) = '{day}'\n",
    "        LIMIT  100000\n",
    "    \"\"\")\n",
    "    if df_raw.empty:\n",
    "        print(f\"No rows for {day}.\"); return\n",
    "\n",
    "    # 3ï¸âƒ£  classify â†’ platform column\n",
    "    df_raw[\"plat\"] = df_raw[\"agent\"].apply(\n",
    "        lambda ua: _split_agent(ua)[0]\n",
    "    )\n",
    "\n",
    "    # 4ï¸âƒ£  counts for each platform  (includes â€œOtherâ€ naturally)\n",
    "    plat_df = (df_raw[\"plat\"]\n",
    "                 .value_counts()\n",
    "                 .reset_index()\n",
    "                 .rename(columns={\"index\": \"Platform\", \"plat\": \"Hits\"}))\n",
    "\n",
    "    total = plat_df[\"Hits\"].sum()\n",
    "\n",
    "    # 5ï¸âƒ£  legend labels\n",
    "    legend_labels = [\n",
    "    f\"{row.Index} â€“ {row.Hits/total*100:.1f}% ({row.Hits:,})\"\n",
    "    for row in plat_df.itertuples()\n",
    "    ]\n",
    "\n",
    "    colours = sns.color_palette(\"viridis\", len(plat_df))\n",
    "\n",
    "    # 6ï¸âƒ£  draw donutâ€‘pie\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    wedges, _ = plt.pie(\n",
    "        plat_df[\"Hits\"],\n",
    "        startangle=90,\n",
    "        colors=colours,\n",
    "        wedgeprops=dict(width=0.4, edgecolor=\"w\"),   # donut style\n",
    "        labels=None                                  # keep slices labelâ€‘free\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Platform Distribution â€“â€¯{day}\",\n",
    "              weight=\"bold\", pad=20)\n",
    "\n",
    "    #  Legend â€“ same colours as wedges\n",
    "    plt.legend(\n",
    "        wedges,\n",
    "        legend_labels,\n",
    "        title=\"Platforms\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        frameon=True,\n",
    "    )\n",
    "\n",
    "    # 7ï¸âƒ£  save (UTC timestamp => cacheâ€‘safe)\n",
    "    if save:\n",
    "        ts    = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        fname = f\"{prefix}_{day}_{ts}.png\"\n",
    "        if static_subdir:\n",
    "            fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "        _save_plot(fname)\n",
    "        print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plt_top_urls_latest_day(db_path: str,\n",
    "                            *,\n",
    "                            top: int = 10,\n",
    "                            save: bool = True,\n",
    "                            prefix: str = \"top_urls_day\",\n",
    "                            static_subdir: str = \"\"):\n",
    "    \"\"\"\n",
    "    Horizontal barâ€‘chart of the TOPâ€‘N mostâ€‘requested URLs\n",
    "    **for the mostâ€‘recent day in `logs`**.\n",
    "\n",
    "    â€¢ Value labels = hit count\n",
    "    â€¢ Saved to  static/<static_subdir>/<prefix>_<YYYYâ€‘MMâ€‘DD>_<UTCts>.png\n",
    "    \"\"\"\n",
    "    # 1ï¸âƒ£  figure out the latest day we have data for\n",
    "    latest_row = _run_sql(db_path,\n",
    "                          \"SELECT DATE(MAX(time)) AS d FROM logs\")\n",
    "    if latest_row.empty or latest_row.iloc[0, 0] is None:\n",
    "        print(\"logs table empty.\"); return\n",
    "    day = latest_row.iloc[0, 0]           # e.g. '2025â€‘07â€‘03'\n",
    "\n",
    "    # 2ï¸âƒ£  query topâ€‘N URLs for that day\n",
    "    df = _run_sql(db_path, f\"\"\"\n",
    "        SELECT url,\n",
    "               COUNT(*) AS hits\n",
    "        FROM   logs\n",
    "        WHERE  DATE(time) = '{day}'\n",
    "        GROUP  BY url\n",
    "        ORDER  BY hits DESC\n",
    "        LIMIT  {top}\n",
    "    \"\"\")\n",
    "    if df.empty:\n",
    "        print(f\"No rows for {day}.\"); return\n",
    "\n",
    "    # 3ï¸âƒ£  plot\n",
    "    plt.figure(figsize=(12, 0.6*len(df)+3))\n",
    "    ax = sns.barplot(data=df, y=\"url\", x=\"hits\",\n",
    "                 hue=\"url\", dodge=False, legend=False,\n",
    "                 edgecolor=\"black\", linewidth=.5,\n",
    "                 palette=sns.color_palette(\"viridis\", len(df)))\n",
    "\n",
    "\n",
    "    # value annotations\n",
    "    for p in ax.patches:\n",
    "        w = p.get_width()\n",
    "        ax.text(w + df[\"hits\"].max()*0.01,\n",
    "                p.get_y() + p.get_height()/2,\n",
    "                f\"{int(w):,}\",\n",
    "                va=\"center\", ha=\"left\")\n",
    "\n",
    "    ax.set(\n",
    "        title=f\"Most Accessed URLs â€“â€¯{day} (Topâ€¯{top})\",\n",
    "        xlabel=\"Hits (requests)\",\n",
    "        ylabel=\"\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 4ï¸âƒ£  save\n",
    "    if save:\n",
    "        ts    = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        fname = f\"{prefix}_{day}_{ts}.png\"\n",
    "        if static_subdir:\n",
    "            fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "        _save_plot(fname)\n",
    "        print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Countryâ€‘level barâ€‘chart â€“ ALL countries for the latest day\n",
    "# ------------------------------------------------------------------\n",
    "def plt_country_req_latest_day(db, *,\n",
    "                               save=True,\n",
    "                               prefix=\"country_requests_day\",\n",
    "                               static_subdir=\"\"):\n",
    "    \"\"\"\n",
    "    Plot hits per *country* for the mostâ€‘recent calendar day appearing\n",
    "    in logs.time.  ALL countries are shown (no LIMIT).\n",
    "\n",
    "    Saved file:  static/<static_subdir>/<prefix>_<YYYYâ€‘MMâ€‘DD>.png\n",
    "    \"\"\"\n",
    "    # â”€â”€ 1.  latest day in the table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    latest_day = _run_sql(db,\n",
    "        \"SELECT DATE(MAX(time)) AS d FROM logs\").iloc[0, 0]\n",
    "    if latest_day is None:\n",
    "        print(\"logs table empty.\"); return\n",
    "\n",
    "    # â”€â”€ 2.  counts per country â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df = _run_sql(db, f\"\"\"\n",
    "        SELECT country, COUNT(*) AS cnt\n",
    "        FROM   logs\n",
    "        WHERE  DATE(time) = '{latest_day}'\n",
    "        GROUP  BY country\n",
    "        ORDER  BY cnt DESC\n",
    "    \"\"\")\n",
    "    if df.empty:\n",
    "        print(f\"No rows for {latest_day}.\"); return\n",
    "\n",
    "    # â”€â”€ 3.  plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    plt.figure(figsize=(12, 0.6*len(df) + 3))\n",
    "    ax = sns.barplot(data=df, y=\"country\", x=\"cnt\",\n",
    "                     edgecolor='black', linewidth=.5)\n",
    "\n",
    "    for p in ax.patches:\n",
    "        w = p.get_width()\n",
    "        ax.text(w + df[\"cnt\"].max()*0.01,\n",
    "                p.get_y() + p.get_height()/2,\n",
    "                f\"{int(w):,}\",\n",
    "                va=\"center\", ha=\"left\")\n",
    "\n",
    "    ax.set(title=f\"Hits by Country â€“â€¯{latest_day} (all countries)\",\n",
    "           xlabel=\"Hits\",\n",
    "           ylabel=\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # â”€â”€ 4.  save (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if save:\n",
    "        ts   = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        fname = f\"{prefix}_{latest_day}_{ts}.png\"\n",
    "        if static_subdir:\n",
    "            fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "        _save_plot(fname)\n",
    "        print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Suspicious IPs by Country â€“ last 30â€¯days, Topâ€‘10\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def plt_suspicious_countries_last30d(db_path: str,\n",
    "                                     *,\n",
    "                                     days: int = 30,\n",
    "                                     top:  int = 10,\n",
    "                                     save: bool = True,\n",
    "                                     prefix: str = \"suspicious_countries_last30d\",\n",
    "                                     static_subdir: str = \"\"):\n",
    "    \"\"\"\n",
    "    Horizontal barâ€‘chart of **unique** suspicious IPs per country\n",
    "    for the last `days` (defaultâ€¯=â€¯30) days of data in `logs`.\n",
    "    \"\"\"\n",
    "    # 1ï¸âƒ£  rollingâ€‘window bounds\n",
    "    latest_day = _run_sql(db_path,\n",
    "                          \"SELECT DATE(MAX(time)) AS d FROM logs\"\n",
    "                         ).iloc[0, 0]\n",
    "    if latest_day is None:\n",
    "        print(\"logs table empty.\"); return\n",
    "    start_day = (datetime.fromisoformat(latest_day)\n",
    "                 - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # 2ï¸âƒ£  DISTINCTâ€‘IP counts  (use the right column!)\n",
    "    df = _run_sql(db_path, f\"\"\"\n",
    "        SELECT l.country,\n",
    "               COUNT(DISTINCT s.suspiciousIp) AS cnt\n",
    "        FROM   ip_suspicious AS s\n",
    "        JOIN   logs           AS l  ON l.ip = s.suspiciousIp\n",
    "        WHERE  DATE(l.time) BETWEEN '{start_day}' AND '{latest_day}'\n",
    "        GROUP  BY l.country\n",
    "        ORDER  BY cnt DESC\n",
    "        LIMIT  {top}\n",
    "    \"\"\")\n",
    "    if df.empty:\n",
    "        print(f\"No suspiciousâ€¯IP rows between {start_day} and {latest_day}.\"); return\n",
    "\n",
    "    # 3ï¸âƒ£  plotting (unchanged)\n",
    "        # 3ï¸âƒ£  plotting\n",
    "    plt.figure(figsize=(12, 0.6*len(df)+3))\n",
    "\n",
    "    # ğŸ”§ <â€‘â€‘â€‘ ONLY THIS LINE CHANGED\n",
    "    ax = sns.barplot(\n",
    "        data=df,\n",
    "        y=\"country\", x=\"cnt\",\n",
    "        hue=\"country\",            # tell Seaborn *which* variable gets the colours\n",
    "        palette=sns.color_palette(\"rocket\", len(df)),\n",
    "        legend=False,             # we donâ€™t need a legend for country names\n",
    "        edgecolor=\"black\", linewidth=.5\n",
    "    )\n",
    "\n",
    "\n",
    "    for p in ax.patches:\n",
    "        w = p.get_width()\n",
    "        ax.text(w + df[\"cnt\"].max()*0.01,\n",
    "                p.get_y() + p.get_height()/2,\n",
    "                f\"{int(w):,}\",\n",
    "                va=\"center\", ha=\"left\")\n",
    "\n",
    "    ax.set(title=f\"Suspicious IPs by Country â€“ lastÂ {days}â€¯days (Topâ€¯{top})\",\n",
    "           xlabel=\"Unique suspicious IPs\",\n",
    "           ylabel=\"\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 4ï¸âƒ£  save\n",
    "    if save:\n",
    "        ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        fname = f\"{prefix}_{latest_day}_{ts}.png\"\n",
    "        if static_subdir:\n",
    "            fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "        _save_plot(fname)\n",
    "        print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Helpers\n",
    "# ------------------------------------------------------------------\n",
    "def _latest_day_with_categories(db_path: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Return the mostâ€‘recent DATE(time) that actually has matching rows\n",
    "    in ip_eachHour_category. Returns None if none found.\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "    SELECT MAX(day) AS d FROM (\n",
    "        SELECT DATE(l.time) AS day\n",
    "        FROM   logs l\n",
    "        JOIN   ip_eachHour_category ic\n",
    "               ON ic.ip = l.ip\n",
    "              AND (\n",
    "                   CAST(strftime('%H', l.time) AS INT) || '-' ||\n",
    "                   CAST((CAST(strftime('%H', l.time) AS INT)+1)%24 AS INT)\n",
    "                  ) = ic.hour\n",
    "    );\n",
    "    \"\"\"\n",
    "    df = _run_sql(db_path, sql)\n",
    "    return df.iloc[0, 0] if not df.empty else None\n",
    "\n",
    "def _join_logs_to_categories(db_path: str, day: str):\n",
    "    \"\"\"\n",
    "    Return a DataFrame [hour, category] for one calendar day.\n",
    "    \"\"\"\n",
    "    return _run_sql(db_path, f\"\"\"\n",
    "        SELECT ic.hour, ic.category\n",
    "        FROM   ip_eachHour_category ic\n",
    "        JOIN   logs l\n",
    "               ON l.ip = ic.ip\n",
    "              AND (\n",
    "                   CAST(strftime('%H', l.time) AS INT) || '-' ||\n",
    "                   CAST((CAST(strftime('%H', l.time) AS INT)+1)%24 AS INT)\n",
    "                  ) = ic.hour\n",
    "        WHERE  DATE(l.time) = '{day}'\n",
    "    \"\"\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Core plotting routine\n",
    "# ------------------------------------------------------------------\n",
    "def save_category_breakdown_one(db_path: str,\n",
    "                                day: str,\n",
    "                                bucket: str = 'All',\n",
    "                                prefix: str = \"category_breakdown\",\n",
    "                                static_subdir: str = \"\"):\n",
    "    \"\"\"\n",
    "    Save ONE barâ€‘chart for (day, bucket) into static/.\n",
    "    bucket = 'All' or e.g. '13-14'\n",
    "    \"\"\"\n",
    "    df = _join_logs_to_categories(db_path, day)\n",
    "    if bucket != 'All':\n",
    "        df = df[df['hour'] == bucket]\n",
    "    if df.empty:\n",
    "        print(f\"[skip] {day} bucket={bucket} has no rows.\"); return\n",
    "\n",
    "    plot_df = (df.groupby('category').size()\n",
    "                 .reset_index(name='cnt')\n",
    "                 .sort_values('cnt', ascending=False))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=plot_df, x='category', y='cnt',\n",
    "                edgecolor='black', linewidth=0.5)\n",
    "    for p in plt.gca().patches:\n",
    "        h = p.get_height()\n",
    "        plt.text(p.get_x()+p.get_width()/2.,\n",
    "                 h + plot_df['cnt'].max()*0.01,\n",
    "                 f'{int(h):,}', ha='center', va='bottom')\n",
    "\n",
    "    slice_txt = f\"{day} â€¢ All hours\" if bucket == 'All' else f\"{day} â€¢ Hour {bucket}\"\n",
    "    plt.title(f\"Traffic Categories â€¢ {slice_txt}\", weight='bold', pad=20)\n",
    "    plt.ylabel(\"Count\"); plt.xlabel(\"Category\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    label = bucket.replace('-', '_') if bucket != 'All' else 'all'\n",
    "    ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "    fname = f\"{prefix}_{day}_{label}_{ts}.png\"\n",
    "    if static_subdir:\n",
    "        fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "    _save_plot(fname)\n",
    "    print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Batch exporter: latest day, all buckets\n",
    "# ------------------------------------------------------------------\n",
    "def export_category_breakdown_latest_day_all_hours(db_path: str,\n",
    "                                                   prefix=\"category_breakdown\",\n",
    "                                                   static_subdir=\"\"):\n",
    "    \"\"\"\n",
    "    Detect the latest calendar day in logs.time that also appears in\n",
    "    ip_eachHour_category, then save:\n",
    "      â€¢ one 'All hours' plot\n",
    "      â€¢ one plot per hour bucket that exists that day.\n",
    "    Put THIS function in PLOTS_TO_RUN.\n",
    "    \"\"\"\n",
    "    latest_day = _latest_day_with_categories(db_path)\n",
    "    if latest_day is None:\n",
    "        print(\"âŒ  No day with category data found.\"); return\n",
    "\n",
    "    df_day = _join_logs_to_categories(db_path, latest_day)\n",
    "    buckets = sorted(df_day['hour'].unique(), key=lambda s: int(s.split('-')[0]))\n",
    "\n",
    "    print(f\"ğŸ“† Exporting category plots for {latest_day} â€¦\")\n",
    "    save_category_breakdown_one(db_path, latest_day, 'All',\n",
    "                                prefix=prefix, static_subdir=static_subdir)\n",
    "    for b in buckets:\n",
    "        save_category_breakdown_one(db_path, latest_day, b,\n",
    "                                    prefix=prefix, static_subdir=static_subdir)\n",
    "    print(\"âœ…  All category plots done.\")\n",
    "\n",
    "\n",
    "\n",
    "def plt_detection_counts_last7d(db_path: str,\n",
    "                                *,\n",
    "                                days:   int  = 7,\n",
    "                                top:    int  = 15,\n",
    "                                save:   bool = True,\n",
    "                                prefix: str  = \"top_suspects_last7d\",\n",
    "                                static_subdir: str = \"\"):\n",
    "    \"\"\"\n",
    "    Barâ€‘chart of IPs with the **most suspicion events** in the last `days`\n",
    "    (default = 7). One event = one row in `ip_suspicious` within that window.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1ï¸âƒ£ Date window\n",
    "    end_day   = datetime.utcnow().date()\n",
    "    start_day = end_day - timedelta(days=days - 1)\n",
    "    sd, ed    = start_day.isoformat(), end_day.isoformat()\n",
    "\n",
    "    # 2ï¸âƒ£ Query suspicion events per IP using correct time column\n",
    "    df = _run_sql(db_path, f\"\"\"\n",
    "        SELECT suspiciousIp   AS ip,\n",
    "               COUNT(*)       AS events\n",
    "        FROM   ip_suspicious\n",
    "        WHERE  DATE(time) BETWEEN '{sd}' AND '{ed}'\n",
    "        GROUP  BY ip\n",
    "        ORDER  BY events DESC\n",
    "        LIMIT  {top}\n",
    "    \"\"\")\n",
    "    if df.empty:\n",
    "        print(f\"No suspicion events between {sd} and {ed}.\"); return\n",
    "\n",
    "    # 3ï¸âƒ£ Plotting\n",
    "    plt.figure(figsize=(12, 0.6*len(df)+3))\n",
    "    ax = sns.barplot(data=df, y=\"ip\", x=\"events\",\n",
    "                 hue=\"ip\", dodge=False,\n",
    "                 edgecolor=\"black\", linewidth=0.5,\n",
    "                 palette=sns.color_palette(\"rocket\", len(df)))\n",
    "\n",
    "\n",
    "    for p, val in zip(ax.patches, df[\"events\"]):\n",
    "        ax.text(p.get_width() + df[\"events\"].max()*0.01,\n",
    "                p.get_y() + p.get_height()/2,\n",
    "                f\"{val:,}\",\n",
    "                va=\"center\", ha=\"left\")\n",
    "\n",
    "    ax.set(\n",
    "        title=f\"Mostâ€¯Flaggedâ€¯IPs â€¢ lastâ€¯{days}â€¯days ({sd}Â â†’Â {ed})\",\n",
    "        xlabel=\"Detection Count\",\n",
    "        ylabel=\"IP Address\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 4ï¸âƒ£ Save\n",
    "    if save:\n",
    "        ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        fname = f\"{prefix}_{ed}_{ts}.png\"\n",
    "        if static_subdir:\n",
    "            fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "        _save_plot(fname)\n",
    "        print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plt_heatmap(db):\n",
    "    \"\"\"Plot heatmap of requests by weekday and hour.\"\"\"\n",
    "    df = _run_sql(db, \"\"\"\n",
    "        SELECT strftime('%w', time) wd, strftime('%H', time) hr, COUNT(*) cnt \n",
    "        FROM logs \n",
    "        GROUP BY wd, hr\"\"\")\n",
    "    if df.empty: return\n",
    "    \n",
    "    # Convert to proper types and pivot\n",
    "    df[\"wd\"] = df[\"wd\"].astype(int)\n",
    "    df[\"hr\"] = df[\"hr\"].astype(int)\n",
    "    pivot = df.pivot(index=\"wd\", columns=\"hr\", values=\"cnt\").fillna(0)\n",
    "    \n",
    "    # Create custom labels for weekdays\n",
    "    weekday_labels = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.heatmap(pivot, cmap=\"YlOrRd\", linewidths=0.5, \n",
    "                xticklabels=range(24), yticklabels=weekday_labels)\n",
    "    \n",
    "    plt.title(\"Request Heatmap: Hourly Activity by Weekday\", weight=\"bold\", pad=20)\n",
    "    plt.ylabel(\"Weekday\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    _save_plot(\"heatmap_hits.png\")\n",
    "\n",
    "def plt_browser_top10_latest_day(db, *,\n",
    "                                 N: int = 10,\n",
    "                                 save=True,\n",
    "                                 prefix=\"browser_requests_top10_day\",\n",
    "                                 static_subdir=\"\"):\n",
    "    \"\"\"\n",
    "    Topâ€‘N browsers (plus 'Other') for the MOSTâ€‘RECENT day\n",
    "    found in logs.time.  Saves to:\n",
    "        static/<static_subdir>/<prefix>_<YYYYâ€‘MMâ€‘DD>.png\n",
    "    \"\"\"\n",
    "    # â”€â”€ 1. detect latest event day â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    latest_day_row = _run_sql(db,\n",
    "        \"SELECT DATE(MAX(time)) AS d FROM logs\")\n",
    "    if latest_day_row.empty or latest_day_row.iloc[0,0] is None:\n",
    "        print(\"logs table empty.\"); return\n",
    "    day = latest_day_row.iloc[0,0]           # e.g. '2025-07-03'\n",
    "\n",
    "    # â”€â”€ 2. pull userâ€‘agents for that day â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df_raw = _run_sql(db, f\"\"\"\n",
    "        SELECT agent FROM logs\n",
    "        WHERE  DATE(time) = '{day}'\n",
    "    \"\"\")\n",
    "    if df_raw.empty:\n",
    "        print(f\"No rows for {day}.\"); return\n",
    "\n",
    "    df_raw[\"browser\"] = df_raw[\"agent\"].apply(_extract_browser_generic)\n",
    "\n",
    "    # â”€â”€ 3. topâ€‘N logic (same as before) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    counts = df_raw[\"browser\"].value_counts()\n",
    "    other_cnt       = counts.get(\"Other\", 0)\n",
    "    counts_no_other = counts.drop(labels=\"Other\", errors=\"ignore\")\n",
    "    topN            = counts_no_other.head(N)\n",
    "\n",
    "    df_plot = (topN.reset_index(name=\"cnt\")\n",
    "                    .rename(columns={\"index\":\"browser\"})\n",
    "                    .sort_values(\"cnt\", ascending=False)\n",
    "                    .reset_index(drop=True))\n",
    "    df_plot = pd.concat(\n",
    "        [df_plot, pd.DataFrame([{\"browser\":\"Other\",\"cnt\":other_cnt}])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    bar_order = df_plot[\"browser\"].tolist()\n",
    "\n",
    "    # â”€â”€ 4. plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    plt.figure(figsize=(12, max(6, .6*len(df_plot))))\n",
    "    sns.barplot(data=df_plot, y=\"browser\", x=\"cnt\",\n",
    "                order=bar_order, edgecolor=\"black\",\n",
    "                linewidth=.5, color=sns.color_palette(\"viridis\",1)[0])\n",
    "\n",
    "    for p in plt.gca().patches:\n",
    "        w = p.get_width()\n",
    "        plt.text(w + df_plot[\"cnt\"].max()*0.02,\n",
    "                 p.get_y()+p.get_height()/2,\n",
    "                 f\"{int(w):,}\", va=\"center\", ha=\"left\")\n",
    "\n",
    "    plt.title(f\"TopÂ {N}Â Browsers HitsÂ ({day})\", weight=\"bold\", pad=15)\n",
    "    plt.xlabel(\"Hits\"); plt.ylabel(\"\")\n",
    "\n",
    "    # â”€â”€ 5. save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if save:\n",
    "        fname = f\"{prefix}_{day}.png\"\n",
    "        if static_subdir:\n",
    "            fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "        _save_plot(fname)\n",
    "        print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "#  Pie / donut of ALL platforms on the latest day  â€“ keeps â€œOtherâ€\n",
    "# -----------------------------------------------------------------\n",
    "def plt_platform_pie_latest_day(db_path: str,\n",
    "                                *,\n",
    "                                save: bool = True,\n",
    "                                prefix: str = \"platform_pie_day\",\n",
    "                                static_subdir: str = \"\"):\n",
    "    \"\"\"\n",
    "    Pie chart of platform share for the mostâ€‘recent calendar day\n",
    "    in `logs.time`, using _split_agent() to classify UA strings.\n",
    "\n",
    "    â€¢ â€œOtherâ€ is kept as its own slice when present.\n",
    "    â€¢ Legend shows  <Platform â€“ x.xâ€¯% (hits)>  with colours matching slices.\n",
    "    â€¢ Saved to  static/<static_subdir>/<prefix>_<YYYYâ€‘MMâ€‘DD>_<UTCts>.png\n",
    "    \"\"\"\n",
    "    # 1ï¸âƒ£  detect latest day in logs\n",
    "    day_row = _run_sql(db_path,\n",
    "        \"SELECT DATE(MAX(time)) AS d FROM logs\")\n",
    "    if day_row.empty or day_row.iloc[0, 0] is None:\n",
    "        print(\"logs table empty.\"); return\n",
    "    day = day_row.iloc[0, 0]                          # e.g. '2025-07-03'\n",
    "\n",
    "    # 2ï¸âƒ£  pull userâ€‘agents for that day (cap rows if huge)\n",
    "    df_raw = _run_sql(db_path, f\"\"\"\n",
    "        SELECT agent\n",
    "        FROM   logs\n",
    "        WHERE  DATE(time) = '{day}'\n",
    "        LIMIT  100000\n",
    "    \"\"\")\n",
    "    if df_raw.empty:\n",
    "        print(f\"No rows for {day}.\"); return\n",
    "\n",
    "    # 3ï¸âƒ£  classify â†’ platform column\n",
    "    df_raw[\"plat\"] = df_raw[\"agent\"].apply(\n",
    "        lambda ua: _split_agent(ua)[0]\n",
    "    )\n",
    "\n",
    "    # 4ï¸âƒ£  counts for each platform  (includes â€œOtherâ€ naturally)\n",
    "    plat_df = (\n",
    "        df_raw[\"plat\"]\n",
    "        .value_counts()\n",
    "        .rename_axis(\"Platform\")  # sets index name\n",
    "        .reset_index(name=\"Hits\") # converts to column + renames count column\n",
    "    )\n",
    "\n",
    "\n",
    "    total = plat_df[\"Hits\"].sum()\n",
    "\n",
    "    # âœ… 5ï¸âƒ£ FIX: generate legend labels without `row.Platform`\n",
    "    legend_labels = [\n",
    "        f\"{row['Platform']} â€“ {row['Hits']/total*100:.1f}% ({row['Hits']:,})\"\n",
    "        for _, row in plat_df.iterrows()\n",
    "    ]\n",
    "\n",
    "    colours = sns.color_palette(\"viridis\", len(plat_df))\n",
    "\n",
    "    # 6ï¸âƒ£  draw donutâ€‘pie\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    wedges, _ = plt.pie(\n",
    "        plat_df[\"Hits\"],\n",
    "        startangle=90,\n",
    "        colors=colours,\n",
    "        wedgeprops=dict(width=0.4, edgecolor=\"w\"),   # donut style\n",
    "        labels=None                                  # keep slices labelâ€‘free\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Platform Distribution â€“â€¯{day}\",\n",
    "              weight=\"bold\", pad=20)\n",
    "\n",
    "    #  Legend â€“ same colours as wedges\n",
    "    plt.legend(\n",
    "        wedges,\n",
    "        legend_labels,\n",
    "        title=\"Platforms\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        frameon=True,\n",
    "    )\n",
    "\n",
    "    # 7ï¸âƒ£  save (UTC timestamp => cacheâ€‘safe)\n",
    "    if save:\n",
    "        ts    = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        fname = f\"{prefix}_{day}_{ts}.png\"\n",
    "        if static_subdir:\n",
    "            fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "        _save_plot(fname)\n",
    "        print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plt_size_vs_status_latest_day(db,\n",
    "                                  *,\n",
    "                                  save=True,\n",
    "                                  prefix=\"size_vs_status_day\",\n",
    "                                  static_subdir=\"\"):\n",
    "    \"\"\"\n",
    "    Scatterâ€‘plot of response size vs. status code\n",
    "    for the latest calendar day present in logs.time.\n",
    "    Each dot = one request (alpha=.6 to reveal density).\n",
    "\n",
    "    Output file:\n",
    "        static/<static_subdir>/<prefix>_<YYYYâ€‘MMâ€‘DD>.png\n",
    "    \"\"\"\n",
    "    # â”€â”€ 1. find the latest event day â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    latest_row = _run_sql(db, \"SELECT DATE(MAX(time)) AS d FROM logs\")\n",
    "    if latest_row.empty or latest_row.iloc[0, 0] is None:\n",
    "        print(\"logs table empty.\"); return\n",
    "    day = latest_row.iloc[0, 0]                       # e.g. '2025â€‘07â€‘03'\n",
    "\n",
    "    # â”€â”€ 2. pull rows for that day â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df = _run_sql(db, f\"\"\"\n",
    "        SELECT status,\n",
    "               CAST(size AS REAL) AS size\n",
    "        FROM   logs\n",
    "        WHERE  DATE(time) = '{day}'\n",
    "          AND  size IS NOT NULL\n",
    "    \"\"\")\n",
    "    if df.empty:\n",
    "        print(f\"No size data for {day}.\"); return\n",
    "\n",
    "    # â”€â”€ 3. plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.scatterplot(data=df,\n",
    "                    x=\"status\", y=\"size\",\n",
    "                    alpha=.6, s=50,\n",
    "                    hue=\"status\", palette=\"viridis\",\n",
    "                    legend=False)\n",
    "\n",
    "    plt.title(f\"Response Size vs. Status Code  â€“Â {day}\",\n",
    "              weight=\"bold\", pad=18)\n",
    "    plt.xlabel(\"HTTP Status Code\")\n",
    "    plt.ylabel(\"Response Size (bytes)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # â”€â”€ 4. save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if save:\n",
    "        fname = f\"{prefix}_{day}.png\"\n",
    "        if static_subdir:\n",
    "            fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "        _save_plot(fname)\n",
    "        print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plt_request_methods(db):\n",
    "    \"\"\"Plot distribution of HTTP request methods.\"\"\"\n",
    "    df = _run_sql(db, \"\"\"\n",
    "        SELECT method, COUNT(*) as count \n",
    "        FROM logs \n",
    "        GROUP BY method \n",
    "        ORDER BY count DESC\"\"\")\n",
    "    if df.empty: return\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(data=df, x=\"method\", y=\"count\", edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value annotations\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width()/2., height + max(df[\"count\"])*0.01,\n",
    "                f'{int(height):,}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title(\"HTTP Request Methods Distribution\", weight=\"bold\", pad=20)\n",
    "    plt.xlabel(\"Method\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    _save_plot(\"request_methods.png\")\n",
    "    \n",
    "def plt_suspicious_reasons(db):\n",
    "    \"\"\"Plot reasons for IPs being marked as suspicious in the last 7 days.\"\"\"\n",
    "\n",
    "    # 1ï¸âƒ£ Get date window from logs table\n",
    "    res = _run_sql(db, \"SELECT MIN(DATE(time)) AS min_day, MAX(DATE(time)) AS max_day FROM logs\")\n",
    "    max_day = res.iloc[0][\"max_day\"]\n",
    "    min_day = (datetime.fromisoformat(max_day) - timedelta(days=6)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # 2ï¸âƒ£ Fetch reason counts for suspicious IPs linked to logs within the last 7 days\n",
    "    df = _run_sql(db, f\"\"\"\n",
    "        SELECT s.reason,\n",
    "               COUNT(*) AS count\n",
    "        FROM   ip_suspicious AS s\n",
    "        JOIN   logs          AS l ON l.ip = s.suspiciousIp\n",
    "        WHERE  DATE(l.time) BETWEEN '{min_day}' AND '{max_day}'\n",
    "        GROUP  BY s.reason\n",
    "        ORDER  BY count DESC\n",
    "    \"\"\")\n",
    "    if df.empty:\n",
    "        print(f\"No suspicious IP activity between {min_day} and {max_day}.\")\n",
    "        return\n",
    "\n",
    "    # 3ï¸âƒ£ Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(data=df, y=\"reason\", x=\"count\", edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        width = p.get_width()\n",
    "        plt.text(width + max(df[\"count\"]) * 0.01,\n",
    "                 p.get_y() + p.get_height() / 2,\n",
    "                 f'{int(width):,}', ha='left', va='center')\n",
    "    \n",
    "    plt.title(f\"Reasons for Suspicious IPs â€“ {min_day} to {max_day}\", weight=\"bold\", pad=20)\n",
    "    plt.xlabel(\"Count(non-unique ips considered)\")\n",
    "    plt.ylabel(\"Reason\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 4ï¸âƒ£ Save\n",
    "    ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "    fname = f\"suspicious_reasons_{min_day}_to_{max_day}_{ts}.png\"\n",
    "    _save_plot(fname)\n",
    "    print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plt_suspects_last15_days_bars(db,\n",
    "                                  *,\n",
    "                                  save=True,\n",
    "                                  prefix=\"suspects_last15d_bars\",\n",
    "                                  static_subdir=\"\"):\n",
    "    \"\"\"\n",
    "    Bar chart: #â€¯of suspicious IPs per calendar day (last 15â€¯days).\n",
    "\n",
    "    â€¢ Missing days are shown as 0.\n",
    "    â€¢ Each bar has its value printed on top.\n",
    "    â€¢ Pastel colour palette for a fresh look.\n",
    "    \"\"\"\n",
    "    # â”€â”€ 1. pull counts from DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df = _run_sql(db, \"\"\"\n",
    "        SELECT DATE(time) AS day, COUNT(*) AS cnt\n",
    "        FROM   ip_suspicious\n",
    "        WHERE  DATE(time) >= DATE('now', '-14 day')\n",
    "        GROUP  BY day\n",
    "        ORDER  BY day\n",
    "    \"\"\")\n",
    "    if df.empty:\n",
    "        print(\"No suspiciousâ€‘IP data in the last 15â€¯days.\"); return\n",
    "\n",
    "    # â”€â”€ 2. ensure every day is present â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    all_days = pd.date_range(end=pd.Timestamp.today().normalize(),\n",
    "                             periods=15, freq=\"D\")\n",
    "    df = (df.set_index(\"day\")\n",
    "            .reindex(all_days.strftime(\"%Y-%m-%d\"), fill_value=0)\n",
    "            .rename_axis(\"day\")\n",
    "            .reset_index())\n",
    "\n",
    "    # â”€â”€ 3. plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    palette = sns.color_palette(\"pastel\", len(df))\n",
    "    ax = sns.barplot(\n",
    "        data=df,\n",
    "        x=\"day\", y=\"cnt\",\n",
    "        hue=\"day\",       \n",
    "        palette=palette,\n",
    "        legend=False,      \n",
    "        edgecolor=\"black\", linewidth=.5\n",
    "    )\n",
    "\n",
    "\n",
    "    # value labels\n",
    "    for p, val in zip(ax.patches, df[\"cnt\"]):\n",
    "        ax.text(p.get_x() + p.get_width()/2,\n",
    "                p.get_height() + max(df[\"cnt\"])*0.02,\n",
    "                f\"{val:,}\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "    ax.set(\n",
    "        title=\"SuspiciousÂ IPs per Day â€“ Lastâ€¯15â€¯Days\",\n",
    "        xlabel=\"Date\",\n",
    "        ylabel=\"Count\"\n",
    "    )\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # â”€â”€ 4. save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if save:\n",
    "        ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        fname = f\"{prefix}_{ts}.png\"\n",
    "        if static_subdir:\n",
    "            fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "        _save_plot(fname)\n",
    "        print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def plt_blocked_ips_latest_day(db, *,\n",
    "                               save=True,\n",
    "                               prefix=\"blocked_ips_day\",\n",
    "                               static_subdir=\"\"):\n",
    "    \"\"\"\n",
    "    Latestâ€‘day blocked IPs barâ€‘chart.\n",
    "      â€¢ colour perâ€‘row by client_block_status\n",
    "      â€¢ label  = client_blocked_at (HH:MM:SS, 24â€‘h)\n",
    "      â€¢ legend colours match bars\n",
    "    \"\"\"\n",
    "    # 1ï¸âƒ£  latest calendar day present\n",
    "    day = _run_sql(db,\n",
    "        \"SELECT DATE(MAX(backend_blocked_at)) AS d FROM blocked_log\"\n",
    "    ).iloc[0, 0]\n",
    "    if day is None:\n",
    "        print(\"blocked_log empty\"); return\n",
    "\n",
    "    # 2ï¸âƒ£  rows for that day\n",
    "    df = _run_sql(db, f\"\"\"\n",
    "        SELECT ip,\n",
    "               detection_count,\n",
    "               COALESCE(TRIM(LOWER(client_block_status)),'â€‘') AS status,\n",
    "               TIME(client_blocked_at)                       AS client_time\n",
    "        FROM   blocked_log\n",
    "        WHERE  DATE(backend_blocked_at) = '{day}'\n",
    "        ORDER  BY detection_count DESC, ip\n",
    "    \"\"\")\n",
    "    if df.empty:\n",
    "        print(f\"No blocked IPs on {day}.\"); return\n",
    "\n",
    "    # 3ï¸âƒ£  status â†’ colour lookâ€‘up\n",
    "    colour_lut = {\n",
    "        'success': '#4caf50',   # green\n",
    "        'ok'     : '#4caf50',\n",
    "        'failed' : '#e53935',   # red\n",
    "        'error'  : '#e53935',\n",
    "        'â€‘'      : '#9e9e9e',   # unknown / blank\n",
    "    }\n",
    "    bar_colours = df['status'].map(lambda s: colour_lut.get(s, '#9e9e9e'))\n",
    "\n",
    "    # 4ï¸âƒ£  plot (no hue; paint each patch afterwards)\n",
    "    plt.figure(figsize=(12, max(6, .6*len(df))))\n",
    "    ax = sns.barplot(\n",
    "        data=df,\n",
    "        y='ip', x='detection_count',\n",
    "        order=df['ip'],            # keep our order\n",
    "        edgecolor='black', linewidth=.5,\n",
    "        color='#ffffff'            # temp colour â€“ will be overwritten\n",
    "    )\n",
    "\n",
    "    # paint each bar, then annotate its time\n",
    "    for bar, colour, t in zip(ax.patches, bar_colours, df['client_time']):\n",
    "        bar.set_facecolor(colour)\n",
    "        ax.text(bar.get_width() + df['detection_count'].max()*0.02,\n",
    "                bar.get_y() + bar.get_height()/2,\n",
    "                t or 'â€“', va='center', ha='left')\n",
    "\n",
    "    # 5ï¸âƒ£  legend â€“ build only for statuses that actually occur\n",
    "    handles, labels = [], []\n",
    "    for st in df['status'].unique():\n",
    "        handles.append(plt.Rectangle((0,0),1,1, fc=colour_lut.get(st,'#9e9e9e'),\n",
    "                                     ec='black', linewidth=.5))\n",
    "        labels.append(st)\n",
    "    ax.legend(handles, labels, title='ClientÂ blockÂ status',\n",
    "              loc='lower right', frameon=True)\n",
    "\n",
    "    ax.set(title=f\"Blocked IPs on {day}\",\n",
    "           xlabel=\"Detection count\", ylabel=\"\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 6ï¸âƒ£  save (timestamp â†’ bypass caching)\n",
    "    if save:\n",
    "        ts    = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        fname = f\"{prefix}_{day}_{ts}.png\"\n",
    "        if static_subdir:\n",
    "            fname = f\"{static_subdir.rstrip('/')}/{fname}\"\n",
    "        _save_plot(fname)\n",
    "        print(f\"âœ” Saved â†’ static/{fname}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# â”€â”€ Generic UA â†’ browser extractor (no hardâ€‘coding) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_skip_tokens = set(\"\"\"\n",
    "mozilla compatible version windows linux applewebkit khtml trident\n",
    "like mobile safari gecko\n",
    "\"\"\".split())\n",
    "\n",
    "_token_re = re.compile(r'([a-z0-9\\+\\-\\.]+)\\/[\\d\\.]+', re.I)\n",
    "\n",
    "def _extract_browser_generic(ua: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the first nonâ€‘generic product token from a UA string.\n",
    "    Falls back to 'Other' when nothing useful is found.\n",
    "    \"\"\"\n",
    "    if not ua:\n",
    "        return \"Other\"\n",
    "\n",
    "    ua_low = ua.lower()\n",
    "\n",
    "    # Look for tokens like Name/1.2.3\n",
    "    for tok in _token_re.findall(ua_low):\n",
    "        if tok not in _skip_tokens and len(tok) > 2:\n",
    "            return tok.capitalize()\n",
    "\n",
    "    # Fallback: any word â‰¥3 chars not in skip list\n",
    "    words = re.findall(r'[a-z][a-z0-9\\+\\-]{2,}', ua_low)\n",
    "    for w in words:\n",
    "        if w not in _skip_tokens:\n",
    "            return w.capitalize()\n",
    "\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# â”€â”€ all other plotting functions stay unchanged â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# (keep them from your previous cell)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "#  RUN EVERYTHING\n",
    "# ----------------------------------------------------------------\n",
    "DB_PATH = \"access_logs.db\"   # adjust if needed\n",
    "\n",
    "PLOTS_TO_RUN = [\n",
    "    plt_suspects_last15_days_bars,\n",
    "    plt_platform_pie_latest_day,\n",
    "    plt_status_pie_latest_day,\n",
    "    plt_top_urls_latest_day,\n",
    "    plt_country_req_latest_day,\n",
    "    plt_suspicious_countries_last30d,\n",
    "    export_category_breakdown_latest_day_all_hours,\n",
    "    plt_avg_size_trend_latest_day,\n",
    "    plt_detection_counts_last7d,\n",
    "    plt_heatmap,\n",
    "    plt_browser_top10_latest_day,\n",
    "    plt_size_vs_status_latest_day,\n",
    "    plt_blocked_ips_latest_day,\n",
    "    plt_request_methods,\n",
    "    plt_suspicious_reasons,\n",
    "]\n",
    "\n",
    "for fn in PLOTS_TO_RUN:\n",
    "    try:\n",
    "        print(f\"Running â†’ {fn.__name__}\", end=\" â€¦ \")\n",
    "        fn(DB_PATH)\n",
    "        print(\"done âœ“\")\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED âœ—  ({e})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21c54f3-3814-4454-8c30-22fafb8e79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Updated refresh_and_detect with enhanced visualizations\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def refresh_and_detect():\n",
    "    \"\"\"\n",
    "    Rebuild derived tables, classify hours, run detections,\n",
    "    enrich Unknown countries, then generate all visualisations.\n",
    "    Includes enhanced plots with better annotations and styling.\n",
    "    \"\"\"\n",
    "    # 1ï¸âƒ£ Country enrichment\n",
    "    GeoCountryUpdater(Config.DB_PATH, \"./resources/geoliteCountry/GeoLite2-Country.mmdb\").run()\n",
    "\n",
    "    # 2ï¸âƒ£ Feature builders & classifiers\n",
    "    builder = AdvancedLogFeatureBuilder(Config.DB_PATH)\n",
    "    analyzer = HourlyHitAnalyzer(Config.DB_PATH)\n",
    "    classifier = EachHourCategoryClassifier(Config.DB_PATH)\n",
    "\n",
    "    builder.run()\n",
    "    analyzer.run_analysis()\n",
    "    classifier.run()\n",
    "\n",
    "    # 3ï¸âƒ£ Generate/refresh all plots with enhanced versions\n",
    "    plot_functions = [\n",
    "        plt_suspects_last15_days_bars,\n",
    "        plt_platform_pie_latest_day,\n",
    "        plt_status_pie_latest_day,\n",
    "        plt_top_urls_latest_day,\n",
    "        plt_country_req_latest_day,\n",
    "        plt_suspicious_countries_last30d,\n",
    "        export_category_breakdown_latest_day_all_hours,\n",
    "        plt_avg_size_trend_latest_day,\n",
    "        plt_detection_counts_last7d,\n",
    "        plt_heatmap,\n",
    "        plt_browser_top10_latest_day,\n",
    "        plt_size_vs_status_latest_day,\n",
    "        plt_blocked_ips_latest_day,\n",
    "        plt_request_methods,\n",
    "        plt_suspicious_reasons,\n",
    "    ]\n",
    "\n",
    "    print(\"Generating visualizations...\")\n",
    "    for i, plot_fn in enumerate(plot_functions, 1):\n",
    "        try:\n",
    "            print(f\"  [{i}/{len(plot_functions)}] {plot_fn.__name__}\", end=\"... \")\n",
    "            plot_fn(Config.DB_PATH)\n",
    "            print(\"âœ“\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nVisualization refresh complete.\")\n",
    "    print(f\"Plots saved to: {STATIC_DIR.absolute()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396fb3f9-f11a-4296-9632-59071e81464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #clearing static folder\n",
    "# import shutil, os\n",
    "\n",
    "# def clean_static_dir(keep_exts=(\".css\", \".js\", \".html\", \".ico\")):\n",
    "#     \"\"\"\n",
    "#     Delete everything in STATIC_DIR except files with extensions you\n",
    "#     explicitly want to keep (default keeps typical web assets).\n",
    "#     â€¢ keep_exts = ()    â†’ nuke absolutely everything.\n",
    "#     \"\"\"\n",
    "#     for item in STATIC_DIR.iterdir():\n",
    "#         if item.is_file():\n",
    "#             if keep_exts and item.suffix.lower() in keep_exts:\n",
    "#                 continue        # skip whitelisted assets\n",
    "#             item.unlink()       # delete file\n",
    "#         elif item.is_dir():\n",
    "#             shutil.rmtree(item) # delete subâ€‘folder recursively\n",
    "#     print(f\"âœ” STATIC_DIR cleaned â†’ {STATIC_DIR}\")\n",
    "\n",
    "# clean_static_dir(keep_exts=())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fcb765a-221a-47f6-8de3-991a598f9234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "\n",
    "# con = sqlite3.connect(\"access_logs.db\")\n",
    "# cursor = con.cursor()\n",
    "# cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "# tables = cursor.fetchall()\n",
    "# con.close()\n",
    "# tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89ef4b0e-9602-42fe-be2d-33ab05202ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# # print_blocked_and_count.py\n",
    "# import sqlite3, pathlib\n",
    "\n",
    "# DB_PATH = pathlib.Path(\"access_logs.db\")   # âŠ  adjust if needed\n",
    "\n",
    "# def pretty_dump(table, conn, order_by=None):\n",
    "#     \"\"\"Print one SQLite table in your preferred style.\"\"\"\n",
    "#     cur  = conn.cursor()\n",
    "#     cols = [row[1] for row in cur.execute(f\"PRAGMA table_info({table});\")]\n",
    "#     print(f\"\\nğŸ“‚ Table: {table}\")\n",
    "#     print(f\"ğŸ”¸ Columns: {cols}\")\n",
    "\n",
    "#     qry  = f\"SELECT * FROM {table}\"\n",
    "#     if order_by:\n",
    "#         qry += f\" ORDER BY {order_by}\"\n",
    "#     for row in cur.execute(qry):\n",
    "#         print(dict(zip(cols, row)))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     if not DB_PATH.exists():\n",
    "#         raise SystemExit(f\"âŒ  DB file not found: {DB_PATH}\")\n",
    "\n",
    "#     with sqlite3.connect(DB_PATH) as conn:\n",
    "#         # â‹  print all rows of blocked_log\n",
    "#         pretty_dump(\"blocked_log\", conn, order_by=\"detected_at\")\n",
    "\n",
    "#         # âŒ  count IDs in logs\n",
    "#         total_ids = conn.execute(\"SELECT COUNT(id) FROM logs;\").fetchone()[0]\n",
    "#         print(f\"\\nğŸ§® Total rows in logs: {total_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57364e09-e613-4f26-a67f-4f59463ab6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ advanced_logs\n",
      "   â†³ columns: ip, req_per_min, unique_urls, error_rate, avg_req_size_bytes, method_ratio_post_by_get, first_time_of_access\n",
      "\n",
      "ğŸ“‚ blocked_log\n",
      "   â†³ columns: ip, detected_at, backend_blocked_at, detection_count, client_blocked_at, client_block_status\n",
      "\n",
      "ğŸ“‚ ddos_multiple_ip\n",
      "   â†³ columns: id, window_start, window_end, duration_s, total_hits, unique_ips, peak_rps, inserted_at\n",
      "\n",
      "ğŸ“‚ ip_eachHour\n",
      "   â†³ columns: ip, 0-1, 1-2, 2-3, 3-4, 4-5, 5-6, 6-7, 7-8, 8-9, 9-10, 10-11, 11-12, 12-13, 13-14, 14-15, 15-16, 16-17, 17-18, 18-19, 19-20, 20-21, 21-22, 22-23, 23-24\n",
      "\n",
      "ğŸ“‚ ip_eachHour_category\n",
      "   â†³ columns: ip, hour, category\n",
      "\n",
      "ğŸ“‚ ip_suspicious\n",
      "   â†³ columns: suspiciousIp, time, reason, detection_count\n",
      "\n",
      "ğŸ“‚ logs\n",
      "   â†³ columns: id, ip, time, method, url, status, size, agent, country, ingest_ts\n",
      "\n",
      "ğŸ“‚ sqlite_sequence\n",
      "   â†³ columns: name, seq\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "def list_tables_and_columns(db_path=\"access_logs.db\"):\n",
    "    \"\"\"\n",
    "    Print every table name plus its column list for the given SQLite DB.\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # fetch all userâ€‘defined tables\n",
    "        cur.execute(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\")\n",
    "        tables = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "        for tbl in tables:\n",
    "            # PRAGMA table_info returns: cid, name, type, notnull, dflt_value, pk\n",
    "            cur.execute(f\"PRAGMA table_info({tbl});\")\n",
    "            cols = [row[1] for row in cur.fetchall()]\n",
    "            print(f\"ğŸ“‚ {tbl}\")\n",
    "            print(\"   â†³ columns:\", \", \".join(cols))\n",
    "            print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    list_tables_and_columns(\"access_logs.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3889c4-93e1-4e88-a0ca-7c4f59e77b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
